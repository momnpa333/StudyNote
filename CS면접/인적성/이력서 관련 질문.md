## 기술 및 설계 관련 질문
### 1. @Async와 Redis INCR 활용 성능 개선 
##### 1. @Async를 쓰게된 이유에 대해 설명해 주세요
특정 유저의 프로필 페이지에 트래픽이 몰리면서 응답 지연 현상이 발생했습니다.  
분석 결과, 조회수 증가 로직이 트랜잭션 안에서 동기적으로 실행되면서 전체 응답 속도에 영향을 주고 있었고, Redis에 접근하여 값을 조회하고 다시 덮어쓰는 방식이었기 때문에 I/O 부하도 컸습니다.

이 문제를 해결하기 위해 `@Async`를 도입해 **조회수 증가 로직을 비동기 이벤트로 분리**했습니다.  
이렇게 함으로써 클라이언트는 실제로 필요한 응답을 빠르게 받을 수 있고, 조회수 처리는 별도로 처리되기 때문에 전체 응답 시간이 단축되었습니다.

특히, `@Async`와 함께 `Redisson Lock` 범위를 최소화하여 동시성 문제도 방지했습니다.  
그 결과 TPS가 약 580 → 1117로, 약 **192% 성능 개선**을 확인할 수 있었습니다.
##### 보충 설명 팁 (면접에서 추가 질문 대비):
> - `@Async`는 Spring에서 기본적으로 `TaskExecutor`를 사용해 별도의 쓰레드에서 작업을 수행하게 합니다.
> - 트랜잭션 안에서 비동기 메서드를 호출하면 새로운 트랜잭션 컨텍스트에서 실행되기 때문에, **비동기 메서드가 트랜잭션 범위에 포함되지 않도록 설계**하는 것이 중요합니다.

1. `@Async`를 사용할 때 트랜잭션 전파는 어떻게 되나요?
**모범 답변 예시:**
> `@Async`는 별도의 스레드에서 비동기적으로 실행되기 때문에, 기본적으로 호출한 쪽의 트랜잭션 컨텍스트를 **공유하지 않습니다.**  
> 즉, 호출한 서비스가 트랜잭션 범위 안에 있더라도, `@Async` 메서드는 **별도의 트랜잭션**에서 실행되며,  
> 트랜잭션 전파 속성(propagation)은 명시적으로 지정해주지 않으면 반영되지 않습니다.
> 
> 이로 인해, `@Async` 메서드에서 DB 작업을 할 경우 트랜잭션이 분리되어 의도하지 않은 결과가 발생할 수 있습니다.  
> 따라서 저는 조회수 증가처럼 **주 트랜잭션과 독립적으로 처리 가능한 작업**에만 `@Async`를 사용하고,  
> 데이터 정합성이 중요한 작업은 `@Async`를 피하고 있습니다.

---
 ✅ 2. 왜 Kafka나 메시지 큐가 아닌 `@Async`를 선택했나요?
**모범 답변 예시:**
> Kafka와 같은 메시지 큐는 대규모 시스템에서 비동기 이벤트를 안정적으로 처리할 수 있는 훌륭한 방법입니다.  
> 하지만 당시 프로젝트는 초기 단계였고, 메시지 지연이나 복잡한 운영 관리를 감당할 만큼의 트래픽은 아니었습니다.  
> 따라서 인프라 부담 없이 빠르게 적용 가능한 `@Async`를 선택했습니다.
> 
> 단순 비동기 처리가 목적이었고, 작업 실패 시 큰 영향을 주지 않는 로직이었기 때문에  
> Spring의 `@Async`로도 충분히 목적을 달성할 수 있었고, 실제로 성능도 크게 개선되었습니다.
> 
> 다만 추후 사용자 수가 증가하고 작업의 중요도가 높아진다면, **Kafka나 RabbitMQ 기반의 비동기 아키텍처로 전환**할 계획도 염두에 두고 있습니다.

 ✅ 3. `@Async`의 한계점은 무엇이라고 생각하시나요?

**모범 답변 예시:**

> `@Async`는 간단하게 비동기 처리를 구현할 수 있는 장점이 있지만, 몇 가지 한계점도 분명히 존재합니다.
> 
> 1. **장애 처리**: 기본적으로 예외를 별도로 핸들링하지 않으면 로그만 출력되며, 메인 흐름에서 인지할 수 없습니다.  
>     → 저는 `AsyncUncaughtExceptionHandler`를 설정해 로그를 남기고, 슬랙 알림을 보내도록 구성했습니다.
>     
> 2. **작업 보장 없음**: `@Async`는 작업이 실패했을 때 재시도나 보장 처리가 내장돼 있지 않습니다.  
>     반면 Kafka는 메시지 큐에 저장되어 재처리할 수 있어 안정성이 더 높습니다.
>     
> 3. **모니터링 어려움**: 쓰레드 풀이나 큐 상태에 대한 실시간 모니터링이 어렵기 때문에, 추후에는 별도의 대시보드 도입이 필요하다고 생각했습니다.
>
##### 2. Redis를 사용한 이유에 대해 설명해 주세요
Redis는 메모리 기반의 인메모리 데이터 저장소로, 읽기·쓰기 속도가 매우 빠르기 때문에 **짧은 시간 안에 반복적으로 접근되는 데이터 처리**에 적합합니다.

WHOKIE 프로젝트에서는 유저 프로필의 **조회수**를 기록하고 있었는데, 이 데이터는 다음과 같은 특징이 있었습니다:

1. **쓰기 빈도는 높지만, 정합성이 약간 느슨해도 되는 데이터**였고
    
2. DB I/O에 부담을 주지 않으면서 빠르게 업데이트할 수 있어야 했습니다.

이러한 특성을 고려했을 때, RDB에 바로 적재하기보다는 Redis를 활용하여 **조회수를 메모리 상에 캐싱하고, 주기적으로 DB에 반영하는 전략**이 적절하다고 판단했습니다.  
특히 `INCR` 명령어를 활용하여 **Race Condition 없이 Atomic하게 조회수 증가가 가능**했고,  
`Redisson Lock`을 통해 **멀티스레드 환경에서도 정합성을 유지**할 수 있었습니다.

이로 인해 TPS가 약 2배 가까이 향상되는 결과를 얻었습니다.  
Redis는 단순 캐시 외에도 분산락, Pub/Sub, 지연 큐 등 다양한 활용도가 있어 선택의 여지가 넓다고 생각합니다.

✅ 보충 설명 (면접관이 깊이 파고들 경우 대비)

**Q. Redis에 저장된 값이 유실될 수 있는데 괜찮나요?**

> 조회수와 같은 데이터는 약간의 유실이 있어도 서비스 품질에 큰 영향이 없다고 판단했습니다.  
> 필요하다면 RDB로 주기적으로 플러시하거나, RDB에 최종 값을 집계해 반영하는 구조로 보완할 수 있습니다.

**Q. Redis 대신 Memcached와 같은 캐시 솔루션은 왜 고려하지 않았나요?**

> Redis는 단순 키-값 저장을 넘어서 다양한 자료구조(list, set, sorted set 등)를 지원하고,  
> 영속성 옵션(RDB/AOF)과 분산락, Lua 스크립트 실행 등이 가능해 훨씬 유연하다고 판단했습니다.
##### 3. 분산락을 사용한 이유에 대해 설명해 주세요
**모범 답변 예시:**
> WHOKIE 프로젝트에서 유저 프로필 조회 시 조회수를 증가시키는 로직을 구현할 때,  
> 동시에 수많은 요청이 몰릴 수 있다는 점에서 **동시성 이슈와 데이터 정합성**이 중요한 고려사항이었습니다.
> 단순히 Redis의 `INCR`만 사용하면 atomic하게 처리되지만, 조회수 외에도 여러 연산이 함께 이뤄질 경우  
> 복합적인 상태 변경에 대한 **동기화 제어**가 필요해졌습니다. 예를 들어:
> 
> - 하루 방문자 수 증가
>     
> - 유니크 IP 여부 확인 후 total count 증가
> 
> 이런 상황에서는 **단일 키를 대상으로 하는 atomic 연산만으로는 불충분**했고,  
> 여러 리소스에 대한 일관된 작업을 보장하기 위해 **Redisson의 분산 락(RLock)**을 도입했습니다.
> 
> 특히 멀티 스레드 환경에서도 정합성을 보장할 수 있고, **락의 범위를 최소화하여 성능 저하 없이** 안정성을 확보할 수 있었습니다.  
> 락 획득 시 재시도 시간과 자동 해제 시간을 명확히 설정하여 데드락이나 자원 점유 문제도 방지했습니다.

### 2. 복합인덱스 사용 
##### 1. 전과 후의 차이에 대해 설명해 주세요

> WHOKIE 프로젝트에서 유저의 답변 목록을 시간순으로 정렬해서 조회하는 API가 있었습니다.  
> 당시 쿼리는 
>WHERE picked_id = 123
  AND created_at BETWEEN '2025-03-01 00:00:00' AND '2025-03-19 23:59:59'
ORDER BY created_at DESC
LIMIT 10; 형태였고,  
> 단일 인덱스로는 `picked`만 설정된 상태였습니다.
> 
> 처음에는 인덱스를 사용했기 때문에 문제가 없다고 생각했지만, 실제 실행 계획(EXPLAIN)을 확인해보니  
> **picked_id로 필터링 후, created_at 기준으로 정렬 시 추가적인 정렬 연산(SORT)**이 발생하고 있었고,  
> 데이터가 많아질수록 정렬 비용 때문에 응답 시간이 점점 느려졌습니다.
> 
> 이를 해결하기 위해 `picked_id, created_at` 순서로 **복합 인덱스**를 설정했습니다.  
> 이렇게 하자 실행 계획에서 `Using index`가 표시되며 **정렬 없이 인덱스 자체로 정렬된 결과를 가져올 수 있었고**,  
> 실제 응답 시간도 약 3.2초에서 0.3초로 90% 이상 개선되었습니다.
> 
> 이 경험을 통해 단순히 인덱스 유무가 아니라, **실제 쿼리와 인덱스 컬럼 순서가 얼마나 정렬/조건에 최적화되었는지**가 중요하다는 것을 실감했습니다.
##### 2. 복합 인덱스를 걸때 단점은 어떻게 생각하는지
복합 인덱스는 성능을 크게 개선해줄 수 있는 도구이지만, 인덱스가 늘어날수록 다음과 같은 단점이 생길 수 있다는 걸 인지하고 있었습니다:

- 디스크 공간 증가
- INSERT/UPDATE 성능 저하
- 쿼리 패턴이 달라지면 무용지물이 되는 구조

그래서 복합 인덱스를 설계할 때 다음과 같은 **보완 전략**을 사용했습니다.
1. **쿼리 패턴 분석을 선행**
> 무작정 인덱스를 추가하지 않고, **API 사용 빈도를 기반으로 자주 사용되는 조회 조건과 정렬 조건**을 먼저 분석했습니다.  
> 실제로 `picked_id`와 `created_at DESC` 조합의 쿼리가 매우 빈번하게 사용된다는 것을 확인하고, 그 조합으로 인덱스를 설계했습니다.

 ✅ 2. **인덱스 수를 최소화하며 충돌 방지**

> covering index, sort index 등 **역할이 겹치는 인덱스는 만들지 않았고**,  
> 조건이 겹치는 컬럼들만 대상으로 복합 인덱스를 구성했습니다.  
> 예를 들어 `group_id`, `created_at`, `picked_id` 모두 필요한 상황에서도,  
> 조건별 우선순위를 나눠서 **복합 인덱스 1~2개로 효율을 최대화**했습니다.

 ✅ 4. **EXPLAIN과 실제 TPS 측정으로 항상 검증**

> 인덱스를 추가할 때마다 반드시 `EXPLAIN ANALYZE`와 nGrinder를 이용해  
> **실제 성능 개선 수치를 확인하고**, 의미 없는 인덱스는 과감히 제거했습니다.

✅ 결론형 멘트

> 결국 인덱스는 “잘 설계하면 이득이 크지만, 남용하면 오히려 독”이 될 수 있다는 걸 실무에서 체감했습니다.  
> 그래서 항상 “쿼리 패턴 기반 설계 → 테스트 기반 검증 → 주기적인 정리”라는 프로세스를 통해 인덱스를 운영했습니다.

### 3. 퍼사드 패턴
##### 퍼사드 패턴을 사용한 이유가 무엇인가요
**모범 답변 예시:**
> WHOKIE 프로젝트가 점차 커지면서 Service 단이 비대해지는 문제가 발생했습니다.  
> 하나의 Service가 여러 도메인에 걸쳐 복잡한 작업을 처리하면서,
> 
> 1. SRP(단일 책임 원칙)가 깨지고,
>     
> 2. 순환 참조 오류 가능성도 생기고,
>     
> 3. 테스트하기도 어려운 구조로 변해가고 있었습니다.
> 특히, `AnswerService` 내부에서 `UserService`, `QuestionService`, `RankingService`, `PointService` 등을 참조하고 있었고,  
> 이를 분리하려 하자 `AService → BService → AService` 구조로 **순환 참조가 발생**할 가능성이 커졌습니다.

✅ 그래서 퍼사드 패턴을 적용했습니다

> 각 도메인에 대해 `ReaderService`와 `WriterService`를 나눠 책임을 분리했고,  
> 그 후, 이들을 **하나의 통합 진입점(MainService or FacadeService)**으로 묶어 상위 서비스에서 사용하는 구조로 개선했습니다.
> 
> 이렇게 하면 개별 서비스는 순환 참조 없이 독립적으로 유지되며,  
> 상위 비즈니스 로직은 **퍼사드만 참조**하면 되므로 코드 의존성도 단순화됩니다.

 ✅ 적용 효과
- ✅ **순환 참조 방지**: Service 간 직접 참조를 줄이고, 구조적으로 순환이 불가능한 형태 유지
- ✅ **비즈니스 로직 응집도 향상**: 퍼사드에서 필요한 도메인들을 모아 조합하면서, 로직 흐름이 명확해짐
- ✅ **단위 테스트 용이**: 개별 Reader/Writer 단위로 테스트가 가능하며, 퍼사드 테스트는 통합 수준으로 분리
- ✅ **유지보수성과 확장성 향상**: 새로운 도메인이 추가되더라도 퍼사드에만 의존관계를 추가하면 되므로 구조가 유연
✅ 결론형 멘트

> 이 경험을 통해 단순히 동작하는 코드를 넘어,  
> **확장성과 유지보수성을 고려한 아키텍처 설계가 서비스 품질에 직접적인 영향을 미친다**는 점을 깊이 이해하게 되었습니다.  
> 이후부터는 도메인 간 의존성이 생기는 순간부터 퍼사드 패턴을 적극 고려하고 있습니다.

##### 질문: 퍼사드 패턴의 단점은 무엇이라고 생각하시나요?
**모범 답변 예시:**
> 퍼사드 패턴은 여러 도메인 서비스를 하나의 진입점으로 묶어, 비즈니스 로직을 명확히 하고 의존성을 단순화할 수 있다는 장점이 있습니다.  
> 하지만 다음과 같은 단점이 존재한다고 생각합니다:

1. **퍼사드 하나에 모든 책임이 집중되면 "슈퍼 서비스"가 될 수 있음**
- 처음엔 깔끔하지만, 시간이 지나면서 모든 기능이 퍼사드로 몰리면

    - **하나의 퍼사드 클래스가 비대해지고**        
    - **SRP(단일 책임 원칙)가 무너지며**
    - 실제로는 또 하나의 **거대한 서비스 객체(Monolithic Service)**가 되는 문제 발생
        

> ✅ **해결 방안**: 퍼사드를 기능 단위로 나누거나, 퍼사드 안에서도 내부 로직을 잘게 분리해 관리합니다. 예: `AnswerFacade`, `UserFacade` 등

🔴 2. **도메인 간 결합도가 퍼사드에 숨어버릴 수 있음**

- 원래 Service A가 Service B를 직접 참조하던 구조에서는 **의존 관계가 코드상 명확**했지만
    
- 퍼사드 안으로 다 감추면 **의존성 구조가 퍼사드 안에 숨겨져 파악이 어려워질 수 있음**
    

> ✅ **해결 방안**: 퍼사드 안에서 사용하는 서비스들의 책임을 명확히 분리(`Writer`, `Reader` 등), 문서화 및 주석 작성 필수

 🔴 4. **오히려 추상화가 너무 과해져서 진입 장벽이 생길 수 있음**
- 퍼사드 안에서 많은 로직이 감춰지다 보면,
    - 신규 개발자가 퍼사드만 보고 내부 처리를 이해하기 어려움
    - 변경 시 영향 범위를 파악하기 힘들 수 있음
        
> ✅ **해결 방안**: 퍼사드 내부의 흐름을 코드 주석이나 문서화로 공유하고,  
> 도메인 단위의 설계 구조를 README 또는 UML로 남기는 습관 필요

 ✅ 결론형 멘트

> 저는 퍼사드 패턴의 핵심은 "적절한 수준의 추상화"라고 생각합니다.  
> 추상화가 지나치면 복잡성이 오히려 높아지므로, **도메인 경계와 비즈니스 흐름을 명확히 정의하고**,  
> 
> 퍼사드 역시 규모가 커지기 전에 적절히 분할하고 문서화하는 방식으로 리스크를 관리하고 있습니다.


### 4. 약 4200만 더미데이터 처리 및 vuser 설정
##### 4200만 더미 데이터를 생성한 이유와, 어떤 방식으로 처리했나요?
**모범 답변 예시:**

> WHOKIE 프로젝트에서 성능 테스트를 본격적으로 진행하기 위해,  
> 경쟁 서비스의 DAU(일일 활성 사용자 수)를 기준으로 시나리오를 설정했고,  
> 이를 기반으로 약 **4200만 건의 더미 데이터를 생성**했습니다.
> 
> 단순히 `INSERT` 문으로 넣을 경우, 외래키 제약과 인덱스 동기화로 인해 **3시간 이상 소요**됐기 때문에,
> 
> - **외래키 제약 조건을 임시로 해제**하고
>     
> - **`LOAD DATA INFILE`**을 사용해 파일 기반 대량 삽입을 수행했습니다.
>     
> - 데이터는 Python 스크립트로 CSV 파일로 생성 후, MySQL에 적재했습니다.
>     
> 
> 그 결과 삽입 시간은 약 **3시간 → 20분**으로 단축됐습니다.

##### vuser는 어떤 기준으로 설정하셨고, 어떤 도구를 사용했나요?

**모범 답변 예시:**

> 성능 테스트에는 **nGrinder**를 사용했습니다.  
> 테스트 시에는 단순히 “많이 넣고 보자”가 아니라, **목표 TPS와 응답 시간**을 기준으로 **현실적인 트래픽 시나리오**를 구성하는 것이 중요하다고 생각했습니다.
> 
> 그래서 경쟁 서비스의 DAU(일일 사용자 수)를 기준으로 다음처럼 계산했습니다:
> 
> - DAU 100만명 → 초당 평균 요청 수 약 1.2 TPS (웹 기준)
> Vuser 구하기

	Vuser = 목표rps x (한번의 시나리오를 완료하는데 걸리는 시간) / (시나리오 당 요청수)  
Vuser = 1155rps x (0.2)/1  
= 231
 Vuser 계산 공식 이유

Vuser는 결국 목표한 트래픽 상황을 만들어 내기 위해 필요.  
따라서 목표한 트래픽 상황을 만들어 냈는지 아닌지를 기준으로 생각할 것

예제

목표 rps : 100rps  
get 요청 1의 목표 응답 시간 : 0.1s  
요청1과 2 사이의 think time : 0.8s  
get 요청 2의 목표 응답 시간 : 0.1s

저희는 100 rps 상황을 만들어야 합니다.  
즉 Vuser들이 1초에 100번의 요청을 보내야 합니다.  
저희가 목표한 응답 시간에 맞추어 응답을 할 수 있다면 Vuser 1명은 1초 (0.1 + 0.8 + 0.1)에 두 번씩 요청보낼 수 있습니다.  
따라서 Vuser가 50명이면 1초 동안 50개의 요청을 보낼 수 있습니다.

공식 자체로 설명해보면,  
Vuser 수 x (1초당 요청 횟수) = rps  
Vuser 수 = rps x ( 1 / (1초당 요청 횟수) )  
Vuser 수 = rps x (요청 하나당 걸리는 평균 시간)  
Vuser 수 = rps x (한번의 시나리오를 완료하는데 걸리는 시간) / (시나리오 당 요청수)   
> - 이 기준을 바탕으로 **200, 500, 1000, 2000 vuser**로 점진적으로 부하를 증가시키며 테스트를 반복했고,
>     
> - CPU 사용률, DB connection pool, GC 로그, TPS, 오류율을 함께 모니터링했습니다.
>     
> 
> 이 과정에서 조회수 증가 API나 랭킹 로직처럼 병목이 발생하는 지점을 발견하고,  
> **비동기 처리와 Redis 활용**, **복합 인덱스 추가**로 병목을 해결했습니다.

##### bulk insert가 아니라 LOAD DATA INFILE을 사용한 이유는?
> 같은 1000만 건을 넣더라도, `BULK INSERT`보다 `LOAD DATA INFILE`이 훨씬 빠른 이유는  
> 단순 SQL 차이가 아니라 **서버 내부의 처리 경로와 최적화 수준이 완전히 다르기 때문**입니다.  
> 다음과 같은 차이점들이 속도 차이를 만들어냅니다:

📌 1. 데이터 전달 방식의 차이

|항목|`BULK INSERT`|`LOAD DATA INFILE`|
|---|---|---|
|전달 방식|클라이언트 → 서버로 SQL 전송|서버가 로컬 디스크에서 직접 읽음|
|네트워크 부하|있음 (SQL 구문 포함)|없음 (네트워크 우회)|
|문자열 파싱|SQL 내 VALUES 파싱 필요|CSV 형식만 빠르게 스캔|

> 📍 `BULK INSERT`는 VALUES(...) 구조를 서버가 **문법 해석**해서 파싱해야 하고,  
> 클라이언트에서 전송된 SQL 문자열을 모두 메모리에 적재해야 하므로 부하가 큽니다.  
> 반면 `LOAD DATA INFILE`은 **파일 포맷만 파싱**하고, 그걸 서버에서 직접 읽기 때문에 **속도가 빠릅니다.**


> 📌 2. 내부 처리 최적화 (버퍼링, 페이지 처리)
- `LOAD DATA INFILE`은 데이터를 **블록 단위로 읽고 쓰는 내부 엔진 최적화**가 적용됩니다.
    
- 반면 `BULK INSERT`는 INSERT 문장을 해석한 뒤 **행 단위로 기록**되므로 처리량이 상대적으로 떨어집니다.
    

> ✅ 특히 InnoDB에서는 **다중 레코드를 한 페이지에 담기 위한 정렬 처리나 트랜잭션 로그 반영**이 다르기 때문에,  
> `LOAD DATA INFILE` 쪽이 **Buffer Pool을 더 효율적으로 사용**합니다.

차이점 1: `LOAD DATA INFILE`은 **블록 단위 처리**로 효율적

- `LOAD DATA INFILE`은 CSV 등 파일을 직접 읽어서 내부적으로 **수천 건 단위의 블록(batch)**으로 한 번에 처리합니다.
    
- InnoDB는 이 데이터를 담기 위해 필요한 페이지들을 메모리에 올려서,  
    **동시에 여러 row를 한 페이지에 쌓아 넣고**, 최소한의 Redo Log로 기록합니다.
    
- 👉 **한 번 페이지에 여러 row**를 정리해서 넣고, **쓰기 I/O 횟수를 최소화**합니다.
    

> 예: 한 페이지(16KB)에 200 row가 들어간다고 가정하면,  
> LOAD DATA INFILE은 100만 row를 삽입할 때 약 5000번의 페이지 갱신만 필요함 (매우 효율적)

---

차이점 2: `BULK INSERT`는 행 단위로 INSERT 해석 후 처리

- `BULK INSERT`는 SQL 구문 자체는 `INSERT INTO ... VALUES (...), (...), ...` 식으로 하나지만,  
    **MySQL 내부적으로는 행 단위로 해석하고**,  
    각 row를 넣을 페이지를 **일일이 검사 및 적재**합니다.
    
- 또한 각 row 삽입마다 Redo/Undo 로그가 개별적으로 처리될 수 있어,  
    **버퍼 풀을 자주 갈아끼우거나**, **I/O 횟수가 증가**합니다.
    

> 즉, InnoDB는 각 row를 개별 삽입 시 **한 페이지에 쌓지 못하고 불필요한 페이지 분할(split)이나 디스크 flush**가 더 자주 발생하게 됩니다.

---
✅ 추가로: AUTO_INCREMENT 관련 처리도 다름

- `LOAD DATA INFILE`은 InnoDB가 **자동 증가 값(AUTO_INCREMENT)**도 **배치 단위로 일괄 증가**시키는 반면,
    
- `INSERT`는 row 단위로 하나씩 증가시켜야 하므로 **AUTO_INCREMENT 락 경합이 더 심할 수 있습니다.**
---

📌 3. Redo/Undo 로그 처리 및 트랜잭션 비용 차이
- `BULK INSERT`는 각 row마다 트랜잭션 로그(Redo Log, Undo Log)를 기록합니다.
- `LOAD DATA INFILE`은 **한 번에 많은 row를 일괄 처리**하거나, 로그를 **최소화**한 방식으로 기록합니다.
    
> ✅ `BULK INSERT`는 쓰기 안정성을 높이기 위해 매 INSERT마다 로그를 기록하지만,  
> `LOAD DATA INFILE`은 **로그 I/O를 줄이기 위한 내부 최적 경로**를 사용하기 때문에 속도 차이가 납니다.

> 👉 테스트 용도라면 `LOAD DATA INFILE`이 빠르고,  
> 👉 운영 환경에서는 `BULK INSERT`의 데이터 정합성 보장이 필요할 수 있습니다.

---

✅ 결론형 멘트

> 요약하자면, `LOAD DATA INFILE`은 단순히 SQL 구문 없이 빠른 것 이상의 의미를 갖습니다.  
> 서버 입장에서는 **네트워크 해석 비용, SQL 파싱, 로그 처리, 버퍼 플러시 비용**까지 줄어들기 때문에,  
> **100만 건 이상**에서는 성능 차이가 분명하게 나타납니다.  
> 저는 약 4200만 건을 삽입할 때, `LOAD DATA INFILE`로 **3시간 → 20분 수준의 성능 향상**을 직접 경험했습니다.
    
#####  Q. nGrinder 외에 다른 성능 테스트 도구는 어떤 걸 알고 있나요?

> nGrinder 외에도 다양한 성능 테스트 도구들이 있고,  
> 각각의 특성과 장단점을 알고 있습니다. 주요 도구는 다음과 같습니다:

 🔧 대표 도구들

|도구|특징|장점|단점|
|---|---|---|---|
|**JMeter**|가장 널리 사용되는 오픈소스 도구|GUI 기반 테스트 시나리오 구성, 커뮤니티 활발|분산 테스트 설정 복잡|
|**Gatling**|Scala 기반 DSL로 시나리오 작성|코드 기반 설정, 결과 시각화 우수|Java보다 진입장벽 높음|
|**Locust**|Python 기반의 경량 도구|시나리오를 코드로 유연하게 작성|높은 TPS 환경에선 제한 있음|
|**k6**|JavaScript 기반 CLI 도구|클라우드 성능 테스트에 최적화, CI/CD 연동 강점|브라우저 기반 테스트는 미지원|

🟢 제가 nGrinder를 선택한 이유


> **분산 Agent 설정이 쉬워서 실제 트래픽 시뮬레이션이 용이**했습니다.

---

**결론 멘트:**

> 프로젝트 특성에 따라 도구를 선택하는 게 중요하다고 생각합니다.  
> nGrinder는 내부 인프라와 잘 맞았지만,  
> 만약 클라우드 기반 API 성능을 CI 파이프라인에서 측정해야 했다면 k6나 Gatling을 선택했을 것 같습니다.

## 협업 및 커뮤니케이션 관련 질문

##### **서비스 설계 중 팀원과 의견이 갈린 경험이 있다면, 어떤 식으로 조율했나요?**
 **모범 답변 예시:**

> WHOKIE 프로젝트에서 **유저 프로필의 방문자 수를 집계하는 로직**을 설계할 때,  
> 팀원과 "RDB만 사용할지, Redis를 병행할지"를 두고 의견이 갈렸던 적이 있습니다.
> 
> 저는 트래픽이 몰릴 경우를 대비해 Redis를 활용한 캐싱 기반 조회수를 제안했고,  
> 팀원은 데이터 정합성과 조회 타이밍이 중요한 지표이기 때문에 RDB만으로 처리해야 한다고 주장했습니다.

🔍 어떻게 조율했나요?

> 먼저 의견 충돌의 핵심이 “속도 vs 정합성”이라는 걸 인식하고,  
> 각자의 관점을 **명확하게 수치와 시나리오로 설명**해보자는 제안을 했습니다.
> 
> - 저는 nGrinder를 통해 TPS 테스트를 실행해 **RDB 단독일 때와 Redis 병행 시의 응답 속도 차이**를 시각화했고,
>     
> - 팀원은 **일일/총 방문자 수의 정확성이 중요한 지표**이며, **캐시 유실 시 데이터 누락 가능성**을 설명해주었습니다.
> 
> 결국 두 방식을 **혼합해서 설계**했습니다:
> 
> - Redis로 방문자 수를 빠르게 증가시켜 사용자 응답을 처리하고,
>     
> - 일정 주기마다 RDB에 동기화하여 정합성도 확보하는 구조로 결정했습니다.
>     

---

✅ 결론형 멘트

> 이 경험을 통해 설계 충돌이 생겼을 때 중요한 건, **누가 맞느냐가 아니라 어떤 기준으로 판단하느냐**라는 걸 깨달았습니다.  
> 저는 그 이후로 **팀 내 의사결정을 감정이 아닌 데이터와 기준 중심**으로 접근하려 노력하고 있습니다.
> 올리브 영에서 협업은 도와주는 것이 아닌 결과를 같이 만들어 나가는 것! 이라는 문장을 보았습니다. 위 경험은 협업을 통해 더 나은 결과를 같이 만들어 가는 경험을 하게 되었습니다.
    
---

## 프로젝트 경험 기반 질문 (WHOKIE 등)
##### 2. **WHOKIE에서 성능 개선을 위해 어떤 메트릭을 기준으로 개선 여부를 판단했나요?**
2. **TPS (Transactions Per Second)**

- 가장 핵심적으로 사용한 메트릭입니다.
    
- nGrinder를 활용해 vuser(가상 사용자) 수를 조절하면서 TPS가 **얼마나 유지되는지**, 부하에 따라 **어디서 병목이 발생하는지**를 측정했습니다.
    
- 예를 들어, **조회수 증가 API의 TPS가 426 → 1046으로 약 245% 개선**된 사례가 있었고,  
    이 수치를 기준으로 성능 개선의 방향성과 효과를 판단했습니다.
    
 2. **평균 응답 시간 (Response Time)**
- 특히 사용자와 직접 맞닿는 API에 대해선 **95th percentile 기준 응답 시간**을 추적했습니다.
    
- 개선 전에는 평균 2.3초였던 응답이 개선 후 0.4초로 줄었고,  
    이를 기반으로 성능 튜닝 효과를 입증했습니다.
    
3. **WHOKIE와 같은 소셜 플랫폼에서 사용자 경험(UX)을 어떻게 고려하며 개발하셨나요?**
    
4. **WHOKIE에서 처리한 더미 데이터가 4200만 건이라고 하셨는데, 이 데이터를 어떻게 생성하고 관리하셨는지 궁금합니다.**
    
## ✅ 문제 해결 역량 관련 질문
#####  **외래키 제약조건 때문에 성능이 저하된 경험이 있다고 하셨는데, 이를 어떻게 해결하셨나요?**
🔍 연관관계를 끊음으로써 얻은 장점

- 락 충돌 제거: 더 이상 User에 S Lock을 걸지 않으므로 데드락 발생 소지가 없음
- 쿼리 명확성 증가: 필요한 경우 ID 기반으로 직접 조회
- 트랜잭션의 독립성 향상: 연관된 엔티티 간 불필요한 의존성 제거

⚠️ Trade-off: 연관관계 제거의 단점

물론 연관관계를 끊는 것은 장점만 있는 것은 아니다.

- 엔티티 간 탐색이 불가능하므로, 항상 명시적 쿼리 혹은 서비스 호출로 데이터를 조회해야 한다.
- 도메인 모델보다는 DB 중심의 설계가 되기 쉬움
- JPA의 영속성 컨텍스트나 cascade, fetch 전략 등을 활용할 수 없음



##### **3시간 이상 소요되던 더미 데이터 처리를 20분으로 줄이기 위해 어떤 기술적 결정을 내리셨나요?**
    
##### **TPS 목표치를 만족하지 못했던 상황에서, 어떤 순서로 문제를 진단하고 해결해 나가셨나요?**
 **모범 답변 예시:**

> WHOKIE 프로젝트에서 **조회수 증가 및 랭킹 API에 대해 초당 1000TPS 이상을 목표**로 설정했었는데,  
> 초기 성능 테스트에서는 약 420TPS에서 **응답 지연과 오류율 증가**가 발생했습니다.
> 
> 이때 저는 다음과 같은 순서로 문제를 진단하고 해결했습니다:

 ① **재현 환경 고정 및 지표 수집**

- nGrinder를 통해 테스트 환경을 고정하고,  
    **TPS, 응답 시간, 오류율, GC 시간, 시스템 CPU 사용량**을 함께 수집했습니다.
    
- 여기서 TPS는 426에서 정체되고 있다는 걸 확인했습니다.
    
② **병목 구간 탐색 (프로파일러)**
- intellij 프로파일러 사용
- db나 분산 락 부분에서 병목현상 확인
- 동시에 `Redisson` 락이 걸린 구간이 넓어지면서 **스레드 경합**도 생기고 있었습니다.
    
 ③ **문제 해결 전략**

- **DB**: `picked_id, created_at DESC` 순서로 복합 인덱스를 재설계하여 `ORDER BY + LIMIT` 성능 개선
    
- **Redisson**: 락을 `read → write` 영역에서만 최소화 적용, 락 획득 시간 제한 추가
    
- **로직 분리**: 정합성과 상관없는 로직은 비동기로 전환하여 응답 시간 단축
    
 ④ **재측정 및 기준 도달 확인**

- 개선 후 다시 TPS 테스트를 진행했고,  
    TPS는 **426 → 1046.1로 약 2.5배 향상**,  
    평균 응답 시간은 2.3초 → 0.4초로 단축, 오류율도 0%를 유지했습니다.
    
✅ 결론형 멘트:

> 이 경험을 통해 성능 문제는 단순히 “어딘가 느리다”는 감각이 아니라,  
> **수치 기반으로 병목을 식별하고, 작은 변경 하나하나가 전체 흐름에 어떤 영향을 주는지 관찰하는 과정**이라는 걸 체감했습니다.  
> 이후부터는 성능 이슈가 발생하면 항상 **지표 수집 → 병목 구간 분석 → 단계별 개선 → 재검증** 흐름으로 대응하고 있습니다.