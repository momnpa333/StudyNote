### 1. 시간복잡도와 공간복잡도에 대해 설명해 주세요.
##### 설명
시간복잡도 (Time Complexity)

시간복잡도는 알고리즘이 문제를 해결하는데 **걸리는 시간**을 입력 크기에 따라 나타낸 것입니다. 주로 Big-O 표기법으로 나타냅니다.

**주요 특징:**
- 알고리즘의 **연산 횟수**를 기준으로 측정합니다
- 입력 크기 n이 커질 때 실행 시간이 어떻게 증가하는지 나타냅니다
- 최선, 평균, 최악의 경우로 나눌 수 있으며, 보통 **최악의 경우**를 기준으로 합니다

**대표적인 시간복잡도:**

- O(1): 상수 시간 - 배열의 인덱스 접근
- O(log n): 로그 시간 - 이진 탐색
- O(n): 선형 시간 - 단순 반복문
- O(n log n): - 효율적인 정렬 (병합 정렬, 퀵 정렬)
- O(n²): 이차 시간 - 이중 반복문 (버블 정렬, 선택 정렬)
- O(2ⁿ): 지수 시간 - 피보나치 재귀

공간복잡도 (Space Complexity)

공간복잡도는 알고리즘이 실행되는 동안 사용하는 **메모리 공간**의 양을 입력 크기에 따라 나타낸 것입니다.

**주요 특징:**

- **고정 공간**: 코드 저장 공간, 상수, 변수 등
- **가변 공간**: 입력 크기에 따라 달라지는 동적 메모리 (배열, 재귀 호출 스택 등)
- 일반적으로 가변 공간만을 고려합니다

**예시:**

- O(1): 추가 메모리를 사용하지 않는 경우 (제자리 정렬)
- O(n): 입력 크기만큼의 추가 배열 사용
- O(log n): 재귀 호출의 깊이가 log n인 경우
- O(n²): 2차원 배열 사용

**시간복잡도와 공간복잡도의 트레이드오프:** 종종 시간을 줄이기 위해 더 많은 메모리를 사용하거나, 메모리를 절약하기 위해 더 많은 시간이 걸리는 상황이 발생합니다. 예를 들어 동적 프로그래밍은 중복 계산을 피하기 위해 메모리에 결과를 저장하여 시간을 단축시킵니다.

##### Big-O, Big-Theta, Big-Omega 에 대해 설명해 주세요.
##### 다른 것을 사용하지 않고, Big-O를 사용하는 이유가 있을까요?
##### O(1)은 O(N^2) 보다 무조건적으로 빠른가요?
### 2. 링크드 리스트에 대해 설명해 주세요.
링크드 리스트 (Linked List)

링크드 리스트는 데이터를 저장하는 선형 자료구조로, 각 데이터 요소(노드)가 **데이터와 다음 노드를 가리키는 포인터**로 구성되어 있습니다.

구조

**노드(Node) 구성:**

- **데이터(Data)**: 실제 저장할 값
- **포인터(Next)**: 다음 노드의 주소를 저장

```
[Data | Next] -> [Data | Next] -> [Data | Next] -> null
```
종류

**1. 단일 연결 리스트 (Singly Linked List)**

- 각 노드가 다음 노드만 가리킵니다
- 한 방향으로만 순회 가능합니다

**2. 이중 연결 리스트 (Doubly Linked List)**

- 각 노드가 이전 노드와 다음 노드를 모두 가리킵니다
- 양방향 순회가 가능합니다

**3. 원형 연결 리스트 (Circular Linked List)**

- 마지막 노드가 첫 번째 노드를 가리킵니다
- 순환 구조를 가집니다

배열과의 비교

**링크드 리스트의 장점:**

- **동적 크기**: 크기를 미리 정할 필요가 없습니다
- **삽입/삭제 효율적**: O(1) - 중간에 데이터를 삽입하거나 삭제할 때 다른 데이터를 이동시킬 필요가 없습니다 (해당 위치를 찾은 후)

**링크드 리스트의 단점:**

- **임의 접근 불가**: 특정 인덱스 접근 시 O(n) - 처음부터 순차적으로 탐색해야 합니다
- **추가 메모리**: 각 노드마다 포인터를 저장해야 하므로 추가 메모리가 필요합니다
- **캐시 효율성 낮음**: 메모리상에 연속적으로 배치되지 않아 캐시 활용도가 낮습니다

시간복잡도

- **접근(Access)**: O(n) - 인덱스로 직접 접근 불가
- **검색(Search)**: O(n) - 순차 탐색 필요
- **삽입(Insert)**: O(1) - 삽입 위치를 알고 있을 때
- **삭제(Delete)**: O(1) - 삭제 위치를 알고 있을 때

공간복잡도

- O(n) - n개의 노드와 각 노드의 포인터

활용 사례

- 크기를 예측할 수 없는 데이터 저장
- 잦은 삽입/삭제가 필요한 경우
- 스택, 큐 구현
- 그래프의 인접 리스트 표현

##### 일반 배열과, 링크드 리스트를 비교해 주세요.
##### 링크드 리스트를 사용해서 구현할 수 있는 다른 자료구조에 대해 설명해 주세요.
링크드 리스트로 구현 가능한 자료구조

링크드 리스트는 다양한 자료구조의 기반이 됩니다. 주요 자료구조들을 살펴보겠습니다.

1. 스택 (Stack)

**특징:**

- LIFO (Last In First Out) 구조
- 마지막에 들어온 데이터가 먼저 나갑니다

**링크드 리스트 구현:**

- 리스트의 head를 스택의 top으로 사용
- push: head에 새 노드 추가 - O(1)
- pop: head 노드 제거 - O(1)
- peek: head 노드 확인 - O(1)

**활용:**

- 함수 호출 스택
- 괄호 검사
- 실행 취소(Undo) 기능

2. 큐 (Queue)

**특징:**

- FIFO (First In First Out) 구조
- 먼저 들어온 데이터가 먼저 나갑니다

**링크드 리스트 구현:**

- head(front)와 tail(rear) 포인터 사용
- enqueue: tail에 새 노드 추가 - O(1)
- dequeue: head 노드 제거 - O(1)

**활용:**

- 프로세스 스케줄링
- BFS(너비 우선 탐색)
- 프린터 대기열

3. 덱 (Deque, Double-ended Queue)

**특징:**

- 양쪽 끝에서 삽입/삭제가 가능한 큐

**링크드 리스트 구현:**

- 이중 연결 리스트로 구현하면 효율적
- 앞/뒤 모두에서 O(1)에 삽입/삭제 가능

**활용:**

- 스택과 큐의 기능을 모두 필요로 할 때
- 슬라이딩 윈도우 문제

4. 해시 테이블 (Hash Table) - 체이닝 방식

**특징:**

- 충돌 해결을 위해 각 버킷에 링크드 리스트 사용

**링크드 리스트 구현:**

- 각 해시 버킷이 링크드 리스트
- 같은 해시값을 가진 데이터들을 연결

**장점:**

- 동적 크기 조정
- 삭제 연산이 간단

5. 그래프 (Graph) - 인접 리스트

**특징:**

- 각 정점의 인접한 정점들을 링크드 리스트로 표현

**링크드 리스트 구현:**

- 각 정점마다 연결된 정점들의 리스트 유지
- 공간복잡도: O(V + E) - V: 정점 수, E: 간선 수

**장점:**

- 희소 그래프(sparse graph)에서 메모리 효율적
- 간선 추가/삭제가 용이

6. LRU 캐시 (Least Recently Used Cache)

**특징:**

- 가장 오래 사용되지 않은 데이터를 제거

**링크드 리스트 구현:**

- 이중 연결 리스트 + 해시맵 조합
- 최근 사용된 데이터를 head로 이동
- tail의 데이터를 제거

**시간복잡도:**

- 조회, 삽입, 삭제 모두 O(1)

7. 우선순위 큐 (Priority Queue) - 비효율적

**특징:**

- 우선순위가 높은 데이터가 먼저 나옵니다

**링크드 리스트 구현:**

- 정렬된 상태로 유지
- 삽입: O(n) - 적절한 위치 찾아야 함
- 삭제: O(1) - head 제거

**단점:**

- 힙(Heap)으로 구현하는 것이 더 효율적 (삽입/삭제 O(log n))

8. 다항식 (Polynomial)

**특징:**

- 각 항을 노드로 표현 (계수, 지수)

**링크드 리스트 구현:**

- 0이 아닌 항만 저장하여 메모리 절약
- 다항식 덧셈, 곱셈 구현 가능

**예시:**

```
3x² + 5x + 2 → [3,2] -> [5,1] -> [2,0]
```
요약

링크드 리스트는 **동적 크기 조정**과 **효율적인 삽입/삭제**가 필요한 자료구조의 기반으로 널리 사용됩니다. 특히 스택, 큐, 그래프의 인접 리스트, 해시 테이블의 체이닝에서 핵심적인 역할을 합니다.
### 3. 스택과 큐에 대해서 설명해 주세요.
##### 스택 2개로 큐를, 큐 2개로 스택을 만드는 방법과, 그 시간복잡도에 대해 설명해 주세요.
스택 2개로 큐 만들기

스택 2개를 사용하면 큐의 FIFO 동작을 구현할 수 있습니다.

기본 아이디어

**두 개의 스택 사용:**

- **Stack1 (input)**: enqueue 연산용
- **Stack2 (output)**: dequeue 연산용

**핵심 원리:** 스택은 LIFO이므로, 데이터를 한 스택에서 다른 스택으로 옮기면 순서가 뒤집힙니다. 두 번 뒤집으면 원래 순서(FIFO)가 됩니다.

구현 방법

**1. Enqueue (삽입)**

- Stack1에 push
- 시간복잡도: O(1)

**2. Dequeue (삭제)**

- Stack2가 비어있으면, Stack1의 모든 요소를 Stack2로 이동
- Stack2에서 pop
- 시간복잡도:
    - 최악: O(n) - Stack1을 모두 옮길 때
    - 평균(amortized): O(1)


##### 시간복잡도를 유지하면서, 배열로 스택과 큐를 구현할 수 있을까요?
안된다. 공간이 차는게 아니라면 가능하다. 그래서 안된다. 
##### Prefix, Infix, Postfix 에 대해 설명하고, 이를 스택을 활용해서 계산/하는 방법에 대해 설명해 주세요.
##### Deque는 어떻게 구현할 수 있을까요?

##### (C++ 한정) Deque의 Random Access 시간복잡도는 O(1) 입니다. 이게 어떻게 가능한걸까요?

### 4. 해시 자료구조에 대해 설명해 주세요.
특정 키로 특정한 값을 바로 찾을수 있게 해주는 자료구조

##### 값이 주어졌을 때, 어떻게 하면 충돌이 최대한 적은 해시 함수를 설계할 수 있을까요?
충돌이 적은 해시 함수 설계 방법

좋은 해시 함수는 충돌을 최소화하고 데이터를 균등하게 분산시켜야 합니다. 다양한 설계 원칙과 기법들을 살펴보겠습니다.

1. 좋은 해시 함수의 조건

**핵심 원칙:**

- **균등 분산(Uniform Distribution)**: 모든 해시값이 거의 동일한 확률로 나와야 함
- **결정성(Deterministic)**: 같은 입력은 항상 같은 해시값
- **효율성**: 계산이 빠를 것
- **눈사태 효과(Avalanche Effect)**: 입력의 작은 변화가 해시값을 크게 변화시킴

2. 데이터 특성 분석

**입력 데이터 파악:**

```
- 데이터 타입: 정수, 문자열, 객체?
- 데이터 범위: 작은 범위? 큰 범위?
- 데이터 분포: 균등? 편향?
- 패턴 유무: 연속적? 규칙적?
```

**예시:**

- 학번(2024001~2024999): 연속적, 패턴 있음
- 전화번호: 앞자리 중복 많음
- 이름: 특정 문자 빈도 높음

 3. 주요 해시 함수 기법

(1) Division Method (나눗셈법)

java

```java
int hash(int key, int tableSize) {
    return key % tableSize;
}
```

**최적화 방법:**

- 테이블 크기를 **소수(prime number)**로 선택
- 2의 거듭제곱은 피할 것 (하위 비트만 사용되어 충돌 증가)

**좋은 예:** tableSize = 97, 101, 211 **나쁜 예:** tableSize = 100, 128, 256

(2) Multiplication Method (곱셈법)

java

```java
int hash(int key, int tableSize) {
    double A = 0.6180339887; // 황금비 (√5 - 1) / 2
    double temp = key * A;
    double fractional = temp - Math.floor(temp);
    return (int)(tableSize * fractional);
}
```

**장점:**

- 테이블 크기에 덜 민감
- A = 0.6180339887 (황금비)가 일반적으로 좋은 성능

(3) Universal Hashing

java

```java
int hash(int key, int tableSize) {
    int a = random_prime(); // 무작위 소수
    int b = random_number(); // 무작위 수
    int p = large_prime;     // 큰 소수
    
    return ((a * key + b) % p) % tableSize;
}
```

**장점:**

- 최악의 경우를 평균적으로 만듦
- 악의적인 입력에 강함

4. 문자열 해시 함수

(1) Polynomial Rolling Hash

java

```java
int hashString(String s, int tableSize) {
    int hash = 0;
    int prime = 31; // 또는 37, 53 등 작은 소수
    
    for (int i = 0; i < s.length(); i++) {
        hash = hash * prime + s.charAt(i);
    }
    
    return Math.abs(hash % tableSize);
}
```

**왜 31을 사용?**

- 작은 소수로 오버플로우 관리 용이
- 31 * x = (x << 5) - x (비트 연산으로 최적화 가능)
- Java String의 hashCode()가 사용

(2) Horner's Method 개선

java

```java
long hashString(String s, int tableSize) {
    long hash = 0;
    long prime = 31;
    long mod = 1_000_000_007; // 큰 소수
    
    for (int i = 0; i < s.length(); i++) {
        hash = (hash * prime + s.charAt(i)) % mod;
    }
    
    return (int)(hash % tableSize);
}
```
5. 복합 키 해시 함수

java

```java
class Person {
    String name;
    int age;
    String city;
    
    @Override
    public int hashCode() {
        int result = 17; // 0이 아닌 초기값
        
        // 각 필드에 다른 소수 곱하기
        result = 31 * result + (name != null ? name.hashCode() : 0);
        result = 31 * result + age;
        result = 31 * result + (city != null ? city.hashCode() : 0);
        
        return result;
    }
}
```

**원칙:**

- 0이 아닌 초기값 사용
- 각 필드마다 소수 곱하기
- null 처리

##### 해시값이 충돌했을 때, 어떤 방식으로 처리할 수 있을까요?
해시 충돌 처리 방법

해시 충돌을 해결하는 방법은 크게 두 가지로 나뉩니다.

---
 1. 체이닝 (Chaining)
개념

각 버킷을 링크드 리스트로 만들어서, 같은 해시값을 가진 데이터들을 연결해서 저장하는 방식입니다.
 동작 방식

- 충돌이 발생하면 해당 버킷의 리스트 끝에 새 노드를 추가합니다
- 검색할 때는 해당 버킷의 리스트를 순회하며 찾습니다
 장점

- 구현이 간단합니다
- 해시 테이블이 가득 차도 계속 데이터를 추가할 수 있습니다
- 삭제 연산이 간단합니다
 단점
- 추가 메모리가 필요합니다 (포인터 저장)
- 최악의 경우 O(n) 시간복잡도를 가집니다 (모든 데이터가 한 버킷에 몰릴 때)
- 캐시 효율이 낮습니다 (메모리가 연속적이지 않음)
 시간복잡도

- 평균: O(1 + α) - α는 Load Factor (n/m)
- 최악: O(n)

---
2. 개방 주소법 (Open Addressing)
 개념

충돌이 발생하면 다른 빈 버킷을 찾아서 저장하는 방식입니다. 테이블 내에서만 해결합니다.

 2-1. 선형 탐사 (Linear Probing)

**동작 방식:**

- 충돌 시 순차적으로 다음 버킷을 확인합니다
- h(k), h(k)+1, h(k)+2, h(k)+3 순서로 탐색

**장점:**

- 구현이 간단합니다
- 캐시 효율이 좋습니다

**단점:**

- Primary Clustering 문제가 발생합니다
- 특정 영역에 데이터가 몰려서 성능이 저하됩니다

**예시:**

- 5번 버킷에 충돌 → 6번 확인 → 7번 확인 → 8번 확인...

2-2. 이차 탐사 (Quadratic Probing)

**동작 방식:**

- 충돌 시 제곱수만큼 떨어진 버킷을 확인합니다
- h(k), h(k)+1², h(k)+2², h(k)+3² 순서로 탐색

**장점:**

- Primary Clustering을 어느 정도 완화합니다

**단점:**

- Secondary Clustering이 발생할 수 있습니다
- 같은 해시값을 가진 키들은 같은 탐사 순서를 따릅니다

**예시:**

- 5번 버킷에 충돌 → 6번(+1) 확인 → 9번(+4) 확인 → 14번(+9) 확인...

2-3. 이중 해싱 (Double Hashing)

**동작 방식:**

- 두 번째 해시 함수를 사용해서 탐사 간격을 결정합니다
- h(k) + i × h₂(k) 순서로 탐색

**장점:**

- Clustering 문제를 가장 잘 해결합니다
- 키마다 다른 탐사 순서를 가집니다

**단점:**

- 두 개의 해시 함수가 필요합니다
- 계산이 조금 더 복잡합니다

**예시:**

- h₁(k) = k % 13
- h₂(k) = 7 - (k % 7)
- 충돌 시 h₂(k)만큼씩 이동

개방 주소법의 공통 특징

**삭제 처리:**

- 단순히 삭제하면 탐사 체인이 끊어집니다
- "deleted" 표시를 사용하거나 재배치가 필요합니다

**Load Factor:**

- 0.7 이하로 유지하는 것이 좋습니다
- 테이블이 가득 차면 리사이징이 필수입니다

---
비교 요약

체이닝 vs 개방 주소법

**체이닝이 좋은 경우:**

- 삭제 연산이 빈번할 때
- Load Factor를 높게 유지해도 괜찮을 때
- 메모리 여유가 있을 때

**개방 주소법이 좋은 경우:**

- 메모리 효율이 중요할 때
- 캐시 성능이 중요할 때
- 데이터 크기가 작을 때

실무에서는

- Java의 HashMap은 체이닝을 사용합니다 (트리로 최적화)
- Python의 dict는 개방 주소법을 사용합니다
- C++의 unordered_map은 체이닝을 사용합니다
##### 본인이 사용하는 언어에서는, 어떤 방식으로 해시 충돌을 처리하나요?
Java의 해시 충돌 처리 방식

Java의 HashMap은 **체이닝 방식**을 기본으로 사용하되, **성능 최적화를 위해 특정 조건에서 트리 구조로 전환**하는 하이브리드 방식을 사용합니다.

---
기본 구조: Separate Chaining

Java 7 이전

- 순수한 체이닝 방식을 사용했습니다
- 각 버킷이 링크드 리스트로 구성되어 있습니다
- 충돌이 발생하면 리스트 끝에 추가합니다
문제점

- 최악의 경우 한 버킷에 모든 데이터가 몰리면 O(n) 성능이 됩니다
- 악의적인 입력으로 DoS 공격이 가능했습니다

---
Java 8 이후: 개선된 방식

핵심 개선사항

**Treeification** - 링크드 리스트를 레드-블랙 트리로 전환합니다.

전환 조건

**리스트 → 트리 전환:**

- 한 버킷의 노드 개수가 **8개 이상**이 되면
- 그리고 전체 테이블 크기가 **64 이상**일 때
- 링크드 리스트를 레드-블랙 트리로 변환합니다

**트리 → 리스트 전환:**

- 노드 개수가 **6개 이하**로 줄어들면
- 다시 링크드 리스트로 변환합니다

왜 8과 6인가?

- 8과 6 사이에 2의 차이를 두어 **히스테리시스**를 만듭니다
- 노드 개수가 7~8 사이를 오가며 빈번한 전환이 발생하는 것을 방지합니다

---
성능 개선 효과

충돌이 많을 때

**링크드 리스트:**

- 검색: O(n)
- 최악의 경우 한 버킷에 n개 노드가 있으면 n번 순회

**레드-블랙 트리:**

- 검색: O(log n)
- 균형 이진 탐색 트리이므로 log n에 탐색

실제 영향

- 악의적인 해시 충돌 공격에 대한 방어가 가능해졌습니다
- 해시 함수가 나빠서 충돌이 많이 발생해도 어느 정도 성능을 보장합니다

---
Load Factor와 리사이징

기본 설정

- 초기 용량: **16**
- Load Factor: **0.75**
- 즉, 12개(16 × 0.75)가 차면 리사이징

리사이징 동작

- 용량을 **2배**로 늘립니다
- 16 → 32 → 64 → 128 → ...
- 모든 데이터를 새로운 테이블로 재배치(rehashing)합니다

Load Factor가 0.75인 이유

- 너무 낮으면: 메모리 낭비
- 너무 높으면: 충돌 증가로 성능 저하
- 0.75는 공간과 성능의 적절한 균형점입니다

---
 해시 함수 개선
hashCode 재가공

Java는 객체의 hashCode를 그대로 사용하지 않고 한 번 더 처리합니다.

**목적:**

- 상위 비트와 하위 비트를 섞어서 분산을 개선합니다
- 특히 테이블 크기가 작을 때 하위 비트만 사용되는 문제를 완화합니다

**방식:**

- hashCode의 상위 16비트를 하위 16비트와 XOR 연산합니다
- 이렇게 하면 상위 비트 정보도 해시값에 반영됩니다

---
실제 동작 예시
정상적인 경우

```
버킷 [0]: [key1] → null
버킷 [1]: [key2] → [key3] → null  (리스트 형태)
버킷 [2]: [key4] → null
...
```
충돌이 많은 경우 (8개 이상)

```
버킷 [1]: TreeNode (레드-블랙 트리)
           /     \
       [key2]   [key5]
        /  \      /  \
    [key3][key4][key6][key7]
           ...
```

---
왜 레드-블랙 트리를 선택했나?

AVL 트리 대신 레드-블랙 트리를 사용하는 이유

**레드-블랙 트리:**

- 삽입/삭제가 더 빠릅니다 (최대 2번의 회전)
- 약간 덜 균형잡혀있지만 충분히 효율적입니다

**AVL 트리:**

- 더 균형잡혀있어 검색은 조금 더 빠릅니다
- 하지만 삽입/삭제 시 더 많은 회전이 필요합니다

HashMap에서는 검색, 삽입, 삭제가 모두 빈번하므로 **레드-블랙 트리가 더 적합**합니다.

---
정리

**Java HashMap의 충돌 처리:**

1. **기본은 체이닝** - 링크드 리스트 사용
2. **성능 최적화** - 충돌이 많으면(8개 이상) 레드-블랙 트리로 전환
3. **동적 조정** - 노드가 줄면(6개 이하) 다시 리스트로 전환
4. **Load Factor 0.75** - 용량의 75%가 차면 2배로 확장
5. **해시 재가공** - hashCode를 한 번 더 처리해서 분산 개선

이러한 설계 덕분에 Java의 HashMap은 평균 O(1), 최악의 경우에도 O(log n)의 성능을 보장하며, 악의적인 충돌 공격에도 어느 정도 방어가 가능합니다.
##### Double Hashing 의 장점과 단점에 대해서 설명하고, 단점을 어떻게 해결할 수 있을지 설명해 주세요.
장점

1. Clustering 문제 해결

**선형 탐사의 문제:**

```
h(k) = k % 10

key 12 → 버킷 [2]
key 22 → 버킷 [2] 충돌 → [3]
key 32 → 버킷 [2] 충돌 → [3] 충돌 → [4]
key 42 → 버킷 [2] 충돌 → [3] 충돌 → [4] 충돌 → [5]

결과: [2][3][4][5] 연속된 영역에 데이터 몰림 (Primary Clustering)
```

**이중 해싱의 해결:**

```
h₁(k) = k % 10
h₂(k) = 7 - (k % 7)

key 12 → 버킷 [2]
key 22 → 버킷 [2] 충돌 → h₂(22) = 7-1 = 6만큼 이동 → [8]
key 32 → 버킷 [2] 충돌 → h₂(32) = 7-4 = 3만큼 이동 → [5]
key 42 → 버킷 [2] 충돌 → h₂(42) = 7-0 = 7만큼 이동 → [9]

결과: [2][5][8][9] 분산됨!
```
2. 키마다 다른 탐사 순서

**이차 탐사의 문제 (Secondary Clustering):**

```
같은 초기 해시값을 가진 키들은 같은 탐사 순서:

key A: hash = 5 → 5, 6, 9, 14, 21...
key B: hash = 5 → 5, 6, 9, 14, 21... (동일!)
```

**이중 해싱:**

```
key A: h₁ = 5, h₂ = 3 → 5, 8, 11, 14, 17...
key B: h₁ = 5, h₂ = 7 → 5, 12, 19, 26, 33...
(완전히 다른 경로!)
```

3. 균등 분산

- 키가 테이블 전체에 골고루 분산됩니다
- 충돌이 발생해도 빠르게 빈 버킷을 찾을 수 있습니다
- 성능이 이론적 최적값에 가깝습니다

---
단점

1. 두 개의 해시 함수 필요

**계산 비용:**

```
선형 탐사: h(k) 한 번만 계산
이중 해싱: h₁(k), h₂(k) 두 번 계산

→ 약 2배의 계산 비용
```

2. 구현 복잡도

**신경 써야 할 것들:**

- 두 해시 함수의 품질 관리
- h₂(k)가 0이 되면 안 됨 (무한 루프)
- h₂(k)와 테이블 크기가 서로소여야 함

**예시 - 잘못된 설계:**

```
테이블 크기: 10
h₂(k) = 2 (항상 2씩 이동)

초기 위치가 홀수면:
1 → 3 → 5 → 7 → 9 → 1 → 3... (짝수 버킷 탐색 불가!)
```

3. 캐시 효율 저하

**선형 탐사:**

```
연속된 메모리 위치 접근
→ 캐시 히트율 높음
→ CPU 캐시 활용 좋음

버킷 [5] → [6] → [7] → [8]
메모리 상에서 인접
```

**이중 해싱:**

```
불규칙한 점프
→ 캐시 미스 발생 가능
→ 메모리 접근 패턴 예측 어려움

버킷 [5] → [12] → [3] → [17]
메모리 상에서 멀리 떨어짐
```

4. 삭제 처리 복잡

**문제:**

```
데이터 삭제 시 "deleted" 마커 필요
→ 탐사 체인이 끊어지면 안 됨
→ 시간이 지날수록 deleted 마커가 쌓임
→ 성능 저하
```

5. Load Factor 제한

**개방 주소법의 공통 문제:**

```
Load Factor가 0.7~0.8 넘으면 급격한 성능 저하
→ 체이닝보다 엄격한 제한
→ 더 자주 리사이징 필요
```

---
단점 해결 방법

 해결책 1: 해시 함수 최적화

**미리 계산된 해시 함수 사용**

```
장점:
- 계산 비용을 줄일 수 있음
- 런타임 오버헤드 감소

방법:
- 비트 연산 활용
- 룩업 테이블 사용
- 간단한 연산으로 h₂ 유도

예시:
h₁(k) = k % m
h₂(k) = 1 + (k % (m-1))  // 간단하면서도 효과적
```

해결책 2: h₂ 설계 원칙 준수

**안전한 h₂ 설계:**

```
규칙 1: h₂(k)는 절대 0이 되면 안 됨
→ h₂(k) = 1 + (k % p) 처럼 최소값 보장

규칙 2: h₂(k)와 테이블 크기가 서로소
→ 테이블 크기를 소수로 선택
→ 또는 h₂(k)가 항상 홀수가 되게 설계

좋은 예:
테이블 크기 m = 소수
h₂(k) = 1 + (k % (m-1))
→ h₂는 1 ~ m-1 범위
→ m과 항상 서로소 관계
```

해결책 3: 하이브리드 접근

**상황에 따라 방법 전환:**

```
Load Factor < 0.5:
→ 선형 탐사 사용 (캐시 효율 활용)

Load Factor ≥ 0.5:
→ 이중 해싱 사용 (충돌 감소)

장점:
- 낮은 부하에서는 캐시 효율
- 높은 부하에서는 충돌 방지
```

해결책 4: 삭제 문제 해결

**방법 A: Lazy Deletion + 주기적 재정리**

```
삭제 시:
1. "deleted" 마커만 표시
2. deleted 비율이 20% 넘으면
   → 전체 재해싱 수행
   → deleted 제거하고 재배치

장점: 간단한 구현
단점: 주기적 오버헤드
```

**방법 B: Robin Hood Hashing**

```
개념:
- 충돌 시 "더 먼 곳에서 온" 키에게 우선권
- 삭제 후 재배치로 빈 공간 최소화

예시:
A는 원래 위치에서 1칸 떨어짐
B는 원래 위치에서 5칸 떨어짐
→ B가 더 "가난"하므로 B를 우선 배치
```

**방법 C: Tombstone Recycling**

```
deleted 위치를 새 데이터 삽입 시 재활용

삽입 알고리즘:
1. 탐사하면서 첫 번째 deleted 위치 기억
2. 빈 버킷 찾으면 deleted 위치에 삽입
3. deleted 마커 정리

효과: deleted가 자연스럽게 줄어듦
```

해결책 5: Load Factor 관리

**적극적인 리사이징:**

```
보수적 임계값:
- Load Factor 0.5~0.6에서 리사이징
- 체이닝(0.75)보다 낮은 기준

점진적 리사이징:
- 한 번에 전체 재해싱하지 않고
- 여러 번의 연산에 걸쳐 점진적으로 이동
- 각 연산의 지연 시간 감소
```

해결책 6: 캐시 친화적 변형

**Hopscotch Hashing:**

```
개념:
- 각 버킷 주변 일정 범위(neighborhood) 내에서만 탐사
- 범위: 보통 32 정도

장점:
- 지역성 보장 (캐시 효율)
- 이중 해싱의 충돌 완화 효과
- 예측 가능한 메모리 접근

예시:
버킷 [10]에 저장 시도
→ [10]~[41] 범위 내에서만 탐색
→ 캐시 라인 안에 들어갈 가능성
```

```
✅ 사용하는 경우:
- 충돌이 많이 예상되는 경우
- 보안이 중요한 경우 (예측 불가능성)
- 균등 분산이 중요한 경우
- Load Factor를 높게 유지해야 할 때

❌ 피하는 경우:
- 캐시 효율이 중요한 경우
- 구현 단순성이 우선인 경우
- 데이터 크기가 매우 작은 경우
- 삭제가 빈번한 경우
```


```
선형 탐사:
+ 구현 간단
+ 캐시 효율 최고
- Primary Clustering

이차 탐사:
+ 구현 비교적 간단
+ Primary Clustering 완화
- Secondary Clustering

이중 해싱:
+ Clustering 문제 완전 해결
+ 최고의 균등 분산
- 구현 복잡
- 캐시 효율 낮음

체이닝:
+ 구현 간단
+ Load Factor 제한 없음
+ 삭제 간단
- 추가 메모리
- 포인터 오버헤드
```

---


**이중 해싱의 핵심:**

**장점 = Clustering 완벽 해결**

- 최고의 충돌 분산 성능
- 이론적 최적값에 근접

**단점 = 구현 복잡도와 캐시**

- 두 해시 함수 관리
- 캐시 효율 저하
- 삭제 처리 복잡

**해결의 핵심:**

- 좋은 h₂ 설계 (0 방지, 서로소 보장)
- 적극적 Load Factor 관리
- 하이브리드 접근 (상황별 전환)
- Hopscotch 같은 변형 사용

**실무에서는:** 대부분 체이닝을 사용합니다. 구현이 간단하고 안정적이며, 이중 해싱의 복잡도가 주는 성능 이득보다 관리 비용이 더 큽니다. 하지만 메모리가 제한적이거나 캐시 지역성보다 균등 분산이 중요한 특수한 경우에는 이중 해싱이 좋은 선택이 될 수 있습니다!
##### Load Factor에 대해 설명해 주세요. 본인이 사용하는 언어에서의 해시 자료구조는 Load Factor에 관련한 정책이 어떻게 구성되어 있나요?
Load Factor (부하 계수)

---
Load Factor란?

정의

```
Load Factor = 저장된 엔트리 수 / 테이블 크기
            = n / m

n = 현재 저장된 데이터 개수
m = 해시 테이블의 버킷 개수
```
의미

**"해시 테이블이 얼마나 차있는가"를 나타내는 비율**

---
Load Factor의 영향

Load Factor가 낮을 때 (예: 0.3)

```
테이블: 100개 버킷
데이터: 30개
Load Factor = 0.3

상태:
[0] A
[1] 비어있음
[2] 비어있음
[3] B
[4] 비어있음
[5] 비어있음
...

장점:
- 충돌 거의 없음
- 검색이 매우 빠름 (거의 O(1))
- 빈 공간 찾기 쉬움

단점:
- 메모리 낭비 (70% 공간이 빔)
```

Load Factor가 높을 때 (예: 0.9)

```
테이블: 100개 버킷
데이터: 90개
Load Factor = 0.9

상태:
[0] A
[1] B → C
[2] D → E → F
[3] G
[4] H → I
[5] J → K → L
...

장점:
- 메모리 효율적 (90% 사용)

단점:
- 충돌 많이 발생
- 체이닝의 리스트가 길어짐
- 검색 속도 저하
- 삽입/삭제 느려짐
```

---
성능과의 관계

체이닝 방식

**평균 탐색 시간:**

```
성공적인 검색: O(1 + α/2)
실패한 검색: O(1 + α)

α = Load Factor

예시:
α = 0.5 → 평균 1.25회 비교
α = 1.0 → 평균 2회 비교
α = 2.0 → 평균 3회 비교
```

개방 주소법

**Load Factor가 1에 가까울수록 급격히 성능 저하**

```
α = 0.5 → 평균 탐사 1.5회
α = 0.7 → 평균 탐사 3.3회
α = 0.9 → 평균 탐사 10회
α = 1.0 → 삽입 불가능!
```

---
Java HashMap의 Load Factor 정책

1. 기본 설정

java

````java
// 기본값
초기 용량(Initial Capacity): 16
Load Factor: 0.75
임계값(Threshold): 16 × 0.75 = 12
```

**의미:**
- 16개 버킷으로 시작
- 12개가 차면 리사이징 발동

### 2. 왜 0.75인가?

**이론적 근거:**
- 시간과 공간의 좋은 균형점
- 포아송 분포 분석 결과
- 0.75에서 충돌 확률과 메모리 효율이 최적

**실험적 결과:**
```
0.5: 메모리 낭비 심함 (50% 사용)
0.75: 균형잡힌 성능
1.0: 충돌 증가로 성능 저하
```

### 3. 리사이징 과정

**Threshold(임계값) 계산:**
```
Threshold = Capacity × Load Factor

예시:
Capacity 16, Load Factor 0.75
→ Threshold = 12

데이터가 12개 이상 들어가면 리사이징
```

**리사이징 동작:**
```
1. 현재 크기의 2배로 확장
   16 → 32 → 64 → 128 → 256 ...

2. 새 테이블 생성

3. 모든 데이터 재해싱(Rehashing)
   - 테이블 크기가 바뀌었으므로
   - hash(key) % 새크기
   - 새로운 버킷에 재배치

4. 새 Threshold 계산
   32 × 0.75 = 24
````

---
Java HashMap 생성자

1. 기본 생성자

java

```java
HashMap<String, Integer> map = new HashMap<>();

// 내부 설정
capacity = 16
loadFactor = 0.75
threshold = 12
```

2. 초기 용량 지정

java

````java
HashMap<String, Integer> map = new HashMap<>(32);

// 내부 설정
capacity = 32
loadFactor = 0.75 (기본값)
threshold = 24
```

**주의사항:**
```
실제로 100개 데이터를 저장할 계획이라면?

나쁜 예:
new HashMap<>(100)
→ 100 × 0.75 = 75에서 리사이징
→ 100개 넣기 전에 리사이징 발생!

좋은 예:
new HashMap<>(100 / 0.75 + 1)
= new HashMap<>(134)
→ 134 × 0.75 = 100.5
→ 100개까지 리사이징 없음
````

3. Load Factor까지 지정

java

````java
HashMap<String, Integer> map = new HashMap<>(32, 0.5f);

// 내부 설정
capacity = 32
loadFactor = 0.5
threshold = 16
```

**언제 사용?**

**Load Factor 낮게 (0.5):**
```
- 검색 속도가 매우 중요할 때
- 메모리 여유가 있을 때
- 충돌을 최소화하고 싶을 때

예: 실시간 검색 시스템
```

**Load Factor 높게 (0.9):**
```
- 메모리가 제한적일 때
- 삽입/삭제보다 저장 공간이 중요할 때

예: 모바일 앱, 임베디드 시스템
````

---
구체적인 동작 예시

시나리오: HashMap 사용

java

````java
HashMap<String, Integer> map = new HashMap<>();
// capacity = 16, loadFactor = 0.75, threshold = 12

// 1~12개 삽입
map.put("A", 1);
map.put("B", 2);
...
map.put("L", 12);
// 상태: size=12, capacity=16
// 아직 리사이징 안 함

// 13번째 삽입
map.put("M", 13);
// size=13 > threshold(12)
// 리사이징 발동!

리사이징 과정:
1. 새 테이블 생성: capacity = 32
2. 12개 데이터 모두 재해싱
3. 새로운 위치에 재배치
4. threshold = 32 × 0.75 = 24
5. "M" 삽입

// 14~24개 삽입
map.put("N", 14);
...
map.put("X", 24);
// 상태: size=24, capacity=32
// 아직 리사이징 안 함

// 25번째 삽입
map.put("Y", 25);
// size=25 > threshold(24)
// 다시 리사이징!
// capacity = 64
```

---

## Load Factor와 성능 측정

### 실험 결과
```
데이터 1000개 삽입 시:

Load Factor 0.5:
- 메모리: 2048 버킷
- 평균 체이닝 길이: 0.5
- 검색 시간: 매우 빠름
- 리사이징 횟수: 많음

Load Factor 0.75 (기본):
- 메모리: 2048 버킷
- 평균 체이닝 길이: 0.75
- 검색 시간: 빠름
- 리사이징 횟수: 적당
- ★ 최적의 균형

Load Factor 1.0:
- 메모리: 1024 버킷
- 평균 체이닝 길이: 1.0
- 검색 시간: 보통
- 리사이징 횟수: 적음
- 충돌 증가 시작
````

---
다른 Java 자료구조의 Load Factor

HashSet

java

```java
// HashMap을 내부적으로 사용
HashSet<String> set = new HashSet<>();

// 동일한 정책
capacity = 16
loadFactor = 0.75
```

ConcurrentHashMap

java

````java
ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();

// 동일한 기본값
capacity = 16
loadFactor = 0.75

// 추가 특징
- 세그먼트별로 독립적인 Load Factor
- 동시성 제어
```

---

## Load Factor 선택 가이드

### Load Factor 0.5 선택
```
✅ 사용하는 경우:
- 검색이 매우 빈번
- 메모리 충분
- 실시간 성능 중요
- 데이터 크기가 예측 가능

예: 캐시 시스템, 실시간 검색
```

### Load Factor 0.75 선택 (권장)
```
✅ 사용하는 경우:
- 대부분의 일반적인 경우
- 균형잡힌 성능 필요
- 표준 웹 애플리케이션
- 특별한 이유가 없을 때

예: 대부분의 애플리케이션
```

### Load Factor 1.0 선택
```
✅ 사용하는 경우:
- 메모리가 제한적
- 삽입/삭제가 적음
- 데이터가 거의 고정
- 읽기 전용에 가까움

예: 설정 파일, 정적 데이터 저장
````

---
주의사항

1. 초기 용량 설정의 중요성

java

```java
// 나쁜 예: 리사이징 여러 번 발생
HashMap<String, Integer> map = new HashMap<>();
for (int i = 0; i < 10000; i++) {
    map.put("key" + i, i);
}
// 리사이징: 16→32→64→128→256→512→1024→2048→4096→8192

// 좋은 예: 리사이징 최소화
int expectedSize = 10000;
int capacity = (int) (expectedSize / 0.75 + 1);
HashMap<String, Integer> map = new HashMap<>(capacity);
// 초기부터 충분한 크기
// 리사이징 0~1회
```

2. Load Factor 변경은 신중하게

java

````java
// 기본값이 보통 최선
HashMap<String, Integer> map = new HashMap<>();

// 특별한 이유 없이 변경하지 말 것
HashMap<String, Integer> map = new HashMap<>(16, 0.9f);
// 충돌 증가, 성능 저하 가능
```

---

## 정리

**Java HashMap의 Load Factor 정책:**

1. **기본값 0.75** - 시간과 공간의 최적 균형
2. **초기 용량 16** - 작은 크기로 시작
3. **2배씩 확장** - 16→32→64→128...
4. **Threshold = Capacity × Load Factor**
5. **size > threshold 시 리사이징**

**핵심 원칙:**
- 대부분 기본값(0.75) 사용
- 예상 데이터 크기를 알면 초기 용량 지정
- 메모리와 성능 중 우선순위에 따라 조정
- 리사이징은 비용이 크므로 최소화

**공식:**
```
적절한 초기 용량 = (예상 데이터 수 / Load Factor) + 1
````
##### 다른 자료구조와 비교하여, 해시 테이블은 멀티스레드 환경에서 심각한 수준의 Race Condition 문제에 빠질 위험이 있습니다. 성능 감소를 최소화 한 채로 해당 문제를 해결할 수 있는 방법을 설계해 보세요.
1. 왜 해시 테이블은 멀티스레드에서 특히 위험한가

해시 테이블은 구조적으로 **공유 상태(shared mutable state)** 가 많습니다.

핵심 위험 포인트

1. **버킷 배열**
    
    - 서로 다른 key라도 같은 bucket으로 해싱될 수 있음
        
2. **충돌 처리 구조**
    
    - linked list / tree 구조 → 중간 삽입·삭제 시 구조 깨짐
        
3. **리사이징**
    
    - rehash 과정에서 전체 테이블 구조 변경
        
    - 동시 접근 시 거의 재앙
        

➡️ 결과적으로

> _읽기/쓰기 모두에서 Race Condition 발생 가능_  
> _심하면 무한 루프, 데이터 유실, JVM crash 수준까지 발생_

---

2. 가장 단순한 해결책과 그 한계

2.1 전체 테이블에 하나의 락

`synchronized put() synchronized get()`

**문제점**

- 모든 연산 직렬화
    
- read-heavy 환경에서도 성능 폭락
    
- 멀티스레드 쓰는 의미가 사라짐
    

➡️ **Correct하지만 쓸 수 없는 설계**

---
3. 성능을 지키는 핵심 아이디어

해결 방향은 딱 세 가지 키워드로 요약됩니다.

> **Lock 분할 + Read/Write 분리 + 구조 변경 최소화**

이걸 어떻게 구현하느냐가 설계의 핵심입니다.

---
4. 설계안 1: Lock Striping (버킷 단위 락 분할)

개념

- 해시 테이블 전체가 아니라 **여러 개의 락으로 쪼갬**
    
- key → hash → 특정 락만 획득
    

`Lock[0] ─ bucket 0,4,8 Lock[1] ─ bucket 1,5,9 Lock[2] ─ bucket 2,6,10 ...`

장점

- 서로 다른 bucket 접근은 병렬 수행 가능
    
- 락 충돌 확률 급감
    
- 구현 난이도 낮음
    

단점

- 같은 락에 매핑된 bucket은 여전히 경쟁
    
- resize 시 여전히 조심 필요
    

➡️ **JDK 7의 ConcurrentHashMap 구조**

---
5. 설계안 2: Read-Write Lock + 불변 읽기

핵심 아이디어

- `get()`은 최대한 락 없이 혹은 read lock만
    
- `put()/remove()`만 write lock
    
적용 방식

- bucket 단위로 `ReentrantReadWriteLock`
    
- read-heavy 환경에서 효과 극대화
    
주의점

- write starvation 위험
    
- read lock 자체도 비용은 있음
    

➡️ **읽기 비중이 압도적으로 높은 캐시 계열에 적합**

---
6. 설계안 3: CAS 기반 Lock-Free 접근 (고급)

핵심 아이디어

- 락 대신 **원자적 연산(CAS)** 사용
    
- bucket entry 교체를 통째로 수행
    

`Node old = table[i] Node new = new Node(key, value, old) CAS(table[i], old, new)`

### 장점

- 락 없음 → 스레드 컨텍스트 스위칭 감소
    
- 높은 처리량
    

### 단점

- 구현 난이도 매우 높음
    
- ABA 문제, 메모리 가시성 고려 필요
    

➡️ **JDK 8+ ConcurrentHashMap이 이 방식 사용**

---

## 7. 리사이징 문제는 어떻게 해결하나?

리사이징은 해시 테이블 Race Condition의 끝판왕입니다.

### 전략

1. **lazy resize**
    
    - 접근 시점에 조금씩 옮김
        
2. **bucket 단위 재해싱**
    
    - 전체 중단 X
        
3. **이중 테이블 유지**
    
    - old → new 점진적 이전
        

➡️ “resize는 한 번에 하지 않는다”가 핵심

---

## 8. 최종 설계 요약 (면접용 정리)

> 해시 테이블은 버킷, 충돌 구조, 리사이징으로 인해 멀티스레드 환경에서 Race Condition에 매우 취약합니다.  
> 이를 해결하기 위해 전체 락 대신 **Lock Striping으로 락을 분할**하고, **읽기/쓰기 경로를 분리**하며, 가능하다면 **CAS 기반의 lock-free 업데이트**를 사용합니다.  
> 또한 리사이징은 점진적으로 수행하여 구조 변경으로 인한 경쟁을 최소화합니다.  
> 이러한 설계를 통해 정합성을 보장하면서도 멀티스레드 환경에서 높은 처리량을 유지할 수 있습니다.

---

## 9. 한 단계 더 잘 대답하려면 (차별화 포인트)

- “그래서 실무에서는 직접 구현하기보다 **ConcurrentHashMap을 사용합니다**”
    
- “쓰기 빈도가 높으면 striping, 읽기 위주면 RWLock”
    
- “극단적 고성능 요구 시 lock-free 구조 고려”

### 5. 트리와 이진트리, 이진탐색트리에 대해 설명해 주세요.

- 그래프와 트리의 차이가 무엇인가요?
- 이진탐색트리에서 중위 탐색을 하게 되면, 그 결과는 어떤 의미를 가지나요?
- 이진탐색트리의 주요 연산에 대한 시간복잡도를 설명하고, 왜 그런 시간복잡도가 도출되는지 설명해 주세요.
- 이진탐색트리의 한계점에 대해 설명해주세요.
- 이진탐색트리의 값 삽입, 삭제 방법에 대해 설명하고, 어떤식으로 값을 삽입하면 편향이 발생할까요?
- 이진탐색트리와 동일한 로직을 사용하면, 삼진탐색트리도 정의할 수 있을까요? 안 된다면, 그 이유에 대해 설명해 주세요.

### 6. 힙에 대해 설명해 주세요.

- 힙을 배열로 구현한다고 가정하면, 어떻게 값을 저장할 수 있을까요?
- 힙의 삽입, 삭제 방식에 대해 설명하고, 왜 이진탐색트리와 달리 편향이 발생하지 않는지 설명해 주세요.
- 힙 정렬의 시간복잡도는 어떻게 되나요? Stable 한가요?

### 7. BBST (Balanced Binary Search Tree) 와, 그 종류에 대해 설명해 주세요.
- Red Black Tree는 어떻게 균형을 유지할 수 있을까요?
- Red Black Tree의 주요 성질 4가지에 대해 설명해 주세요.
- 2-3-4 Tree, AVL Tree 등의 다른 BBST 가 있음에도, 왜 Red Black Tree가 많이 사용될까요?

### 8. 정렬 알고리즘에 대해 설명해 주세요.
- Quick Sort와 Merge Sort를 비교해 주세요.
- Quick Sort에서 O(N^2)이 걸리는 예시를 들고, 이를 개선할 수 있는 방법에 대해 설명해 주세요.
- Stable Sort가 무엇이고, 어떤 정렬 알고리즘이 Stable 한지 설명해 주세요.
- Merge Sort를 재귀를 사용하지 않고 구현할 수 있을까요?
- Radix Sort에 대해 설명해 주세요.
- Bubble, Selection, Insertion Sort의 속도를 비교해 주세요.
- 값이 **거의** 정렬되어 있거나, 아예 정렬되어 있다면, 위 세 알고리즘의 성능 비교 결과는 달라질까요?
- 본인이 사용하고 있는 언어에선, 어떤 정렬 알고리즘을 사용하여 정렬 함수를 제공하고 있을까요?
- 정렬해야 하는 데이터는 50G 인데, 메모리가 4G라면, 어떤 방식으로 정렬을 진행할 수 있을까요?