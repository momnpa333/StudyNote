### 1. 시간복잡도와 공간복잡도에 대해 설명해 주세요.
##### 설명
시간복잡도 (Time Complexity)

시간복잡도는 알고리즘이 문제를 해결하는데 **걸리는 시간**을 입력 크기에 따라 나타낸 것입니다. 주로 Big-O 표기법으로 나타냅니다.

**주요 특징:**
- 알고리즘의 **연산 횟수**를 기준으로 측정합니다
- 입력 크기 n이 커질 때 실행 시간이 어떻게 증가하는지 나타냅니다
- 최선, 평균, 최악의 경우로 나눌 수 있으며, 보통 **최악의 경우**를 기준으로 합니다

**대표적인 시간복잡도:**

- O(1): 상수 시간 - 배열의 인덱스 접근
- O(log n): 로그 시간 - 이진 탐색
- O(n): 선형 시간 - 단순 반복문
- O(n log n): - 효율적인 정렬 (병합 정렬, 퀵 정렬)
- O(n²): 이차 시간 - 이중 반복문 (버블 정렬, 선택 정렬)
- O(2ⁿ): 지수 시간 - 피보나치 재귀

공간복잡도 (Space Complexity)

공간복잡도는 알고리즘이 실행되는 동안 사용하는 **메모리 공간**의 양을 입력 크기에 따라 나타낸 것입니다.

**주요 특징:**

- **고정 공간**: 코드 저장 공간, 상수, 변수 등
- **가변 공간**: 입력 크기에 따라 달라지는 동적 메모리 (배열, 재귀 호출 스택 등)
- 일반적으로 가변 공간만을 고려합니다

**예시:**

- O(1): 추가 메모리를 사용하지 않는 경우 (제자리 정렬)
- O(n): 입력 크기만큼의 추가 배열 사용
- O(log n): 재귀 호출의 깊이가 log n인 경우
- O(n²): 2차원 배열 사용

**시간복잡도와 공간복잡도의 트레이드오프:** 종종 시간을 줄이기 위해 더 많은 메모리를 사용하거나, 메모리를 절약하기 위해 더 많은 시간이 걸리는 상황이 발생합니다. 예를 들어 동적 프로그래밍은 중복 계산을 피하기 위해 메모리에 결과를 저장하여 시간을 단축시킵니다.

##### Big-O, Big-Theta, Big-Omega 에 대해 설명해 주세요.
##### 다른 것을 사용하지 않고, Big-O를 사용하는 이유가 있을까요?
##### O(1)은 O(N^2) 보다 무조건적으로 빠른가요?
### 2. 링크드 리스트에 대해 설명해 주세요.
##### 설명
링크드 리스트 (Linked List)

링크드 리스트는 데이터를 저장하는 선형 자료구조로, 각 데이터 요소(노드)가 **데이터와 다음 노드를 가리키는 포인터**로 구성되어 있습니다.

구조

**노드(Node) 구성:**

- **데이터(Data)**: 실제 저장할 값
- **포인터(Next)**: 다음 노드의 주소를 저장

```
[Data | Next] -> [Data | Next] -> [Data | Next] -> null
```
종류

**1. 단일 연결 리스트 (Singly Linked List)**

- 각 노드가 다음 노드만 가리킵니다
- 한 방향으로만 순회 가능합니다

**2. 이중 연결 리스트 (Doubly Linked List)**

- 각 노드가 이전 노드와 다음 노드를 모두 가리킵니다
- 양방향 순회가 가능합니다

**3. 원형 연결 리스트 (Circular Linked List)**

- 마지막 노드가 첫 번째 노드를 가리킵니다
- 순환 구조를 가집니다

배열과의 비교

**링크드 리스트의 장점:**

- **동적 크기**: 크기를 미리 정할 필요가 없습니다
- **삽입/삭제 효율적**: O(1) - 중간에 데이터를 삽입하거나 삭제할 때 다른 데이터를 이동시킬 필요가 없습니다 (해당 위치를 찾은 후)

**링크드 리스트의 단점:**

- **임의 접근 불가**: 특정 인덱스 접근 시 O(n) - 처음부터 순차적으로 탐색해야 합니다
- **추가 메모리**: 각 노드마다 포인터를 저장해야 하므로 추가 메모리가 필요합니다
- **캐시 효율성 낮음**: 메모리상에 연속적으로 배치되지 않아 캐시 활용도가 낮습니다

시간복잡도

- **접근(Access)**: O(n) - 인덱스로 직접 접근 불가
- **검색(Search)**: O(n) - 순차 탐색 필요
- **삽입(Insert)**: O(1) - 삽입 위치를 알고 있을 때
- **삭제(Delete)**: O(1) - 삭제 위치를 알고 있을 때

공간복잡도

- O(n) - n개의 노드와 각 노드의 포인터

활용 사례

- 크기를 예측할 수 없는 데이터 저장
- 잦은 삽입/삭제가 필요한 경우
- 스택, 큐 구현
- 그래프의 인접 리스트 표현

##### 일반 배열과, 링크드 리스트를 비교해 주세요.
##### 링크드 리스트를 사용해서 구현할 수 있는 다른 자료구조에 대해 설명해 주세요.
링크드 리스트로 구현 가능한 자료구조

링크드 리스트는 다양한 자료구조의 기반이 됩니다. 주요 자료구조들을 살펴보겠습니다.

1. 스택 (Stack)

**특징:**

- LIFO (Last In First Out) 구조
- 마지막에 들어온 데이터가 먼저 나갑니다

**링크드 리스트 구현:**

- 리스트의 head를 스택의 top으로 사용
- push: head에 새 노드 추가 - O(1)
- pop: head 노드 제거 - O(1)
- peek: head 노드 확인 - O(1)

**활용:**

- 함수 호출 스택
- 괄호 검사
- 실행 취소(Undo) 기능

2. 큐 (Queue)

**특징:**

- FIFO (First In First Out) 구조
- 먼저 들어온 데이터가 먼저 나갑니다

**링크드 리스트 구현:**

- head(front)와 tail(rear) 포인터 사용
- enqueue: tail에 새 노드 추가 - O(1)
- dequeue: head 노드 제거 - O(1)

**활용:**

- 프로세스 스케줄링
- BFS(너비 우선 탐색)
- 프린터 대기열

3. 덱 (Deque, Double-ended Queue)

**특징:**

- 양쪽 끝에서 삽입/삭제가 가능한 큐

**링크드 리스트 구현:**

- 이중 연결 리스트로 구현하면 효율적
- 앞/뒤 모두에서 O(1)에 삽입/삭제 가능

**활용:**

- 스택과 큐의 기능을 모두 필요로 할 때
- 슬라이딩 윈도우 문제

4. 해시 테이블 (Hash Table) - 체이닝 방식

**특징:**

- 충돌 해결을 위해 각 버킷에 링크드 리스트 사용

**링크드 리스트 구현:**

- 각 해시 버킷이 링크드 리스트
- 같은 해시값을 가진 데이터들을 연결

**장점:**

- 동적 크기 조정
- 삭제 연산이 간단

5. 그래프 (Graph) - 인접 리스트

**특징:**

- 각 정점의 인접한 정점들을 링크드 리스트로 표현

**링크드 리스트 구현:**

- 각 정점마다 연결된 정점들의 리스트 유지
- 공간복잡도: O(V + E) - V: 정점 수, E: 간선 수

**장점:**

- 희소 그래프(sparse graph)에서 메모리 효율적
- 간선 추가/삭제가 용이

6. LRU 캐시 (Least Recently Used Cache)

**특징:**

- 가장 오래 사용되지 않은 데이터를 제거

**링크드 리스트 구현:**

- 이중 연결 리스트 + 해시맵 조합
- 최근 사용된 데이터를 head로 이동
- tail의 데이터를 제거

**시간복잡도:**

- 조회, 삽입, 삭제 모두 O(1)

7. 우선순위 큐 (Priority Queue) - 비효율적

**특징:**

- 우선순위가 높은 데이터가 먼저 나옵니다

**링크드 리스트 구현:**

- 정렬된 상태로 유지
- 삽입: O(n) - 적절한 위치 찾아야 함
- 삭제: O(1) - head 제거

**단점:**

- 힙(Heap)으로 구현하는 것이 더 효율적 (삽입/삭제 O(log n))

8. 다항식 (Polynomial)

**특징:**

- 각 항을 노드로 표현 (계수, 지수)

**링크드 리스트 구현:**

- 0이 아닌 항만 저장하여 메모리 절약
- 다항식 덧셈, 곱셈 구현 가능

**예시:**

```
3x² + 5x + 2 → [3,2] -> [5,1] -> [2,0]
```
요약

링크드 리스트는 **동적 크기 조정**과 **효율적인 삽입/삭제**가 필요한 자료구조의 기반으로 널리 사용됩니다. 특히 스택, 큐, 그래프의 인접 리스트, 해시 테이블의 체이닝에서 핵심적인 역할을 합니다.
### 3. 스택과 큐에 대해서 설명해 주세요.
##### 스택 2개로 큐를, 큐 2개로 스택을 만드는 방법과, 그 시간복잡도에 대해 설명해 주세요.
스택 2개로 큐 만들기

스택 2개를 사용하면 큐의 FIFO 동작을 구현할 수 있습니다.

기본 아이디어

**두 개의 스택 사용:**

- **Stack1 (input)**: enqueue 연산용
- **Stack2 (output)**: dequeue 연산용

**핵심 원리:** 스택은 LIFO이므로, 데이터를 한 스택에서 다른 스택으로 옮기면 순서가 뒤집힙니다. 두 번 뒤집으면 원래 순서(FIFO)가 됩니다.

구현 방법

**1. Enqueue (삽입)**

- Stack1에 push
- 시간복잡도: O(1)

**2. Dequeue (삭제)**

- Stack2가 비어있으면, Stack1의 모든 요소를 Stack2로 이동
- Stack2에서 pop
- 시간복잡도:
    - 최악: O(n) - Stack1을 모두 옮길 때
    - 평균(amortized): O(1)


##### 시간복잡도를 유지하면서, 배열로 스택과 큐를 구현할 수 있을까요?
안된다. 공간이 차는게 아니라면 가능하다. 그래서 안된다. 
##### Prefix, Infix, Postfix 에 대해 설명하고, 이를 스택을 활용해서 계산/하는 방법에 대해 설명해 주세요.
##### Deque는 어떻게 구현할 수 있을까요?

##### (C++ 한정) Deque의 Random Access 시간복잡도는 O(1) 입니다. 이게 어떻게 가능한걸까요?

### 4. 해시 자료구조에 대해 설명해 주세요.
특정 키로 특정한 값을 바로 찾을수 있게 해주는 자료구조

##### 값이 주어졌을 때, 어떻게 하면 충돌이 최대한 적은 해시 함수를 설계할 수 있을까요?
충돌이 적은 해시 함수 설계 방법

좋은 해시 함수는 충돌을 최소화하고 데이터를 균등하게 분산시켜야 합니다. 다양한 설계 원칙과 기법들을 살펴보겠습니다.

1. 좋은 해시 함수의 조건

**핵심 원칙:**

- **균등 분산(Uniform Distribution)**: 모든 해시값이 거의 동일한 확률로 나와야 함
- **결정성(Deterministic)**: 같은 입력은 항상 같은 해시값
- **효율성**: 계산이 빠를 것
- **눈사태 효과(Avalanche Effect)**: 입력의 작은 변화가 해시값을 크게 변화시킴

2. 데이터 특성 분석

**입력 데이터 파악:**

```
- 데이터 타입: 정수, 문자열, 객체?
- 데이터 범위: 작은 범위? 큰 범위?
- 데이터 분포: 균등? 편향?
- 패턴 유무: 연속적? 규칙적?
```

**예시:**

- 학번(2024001~2024999): 연속적, 패턴 있음
- 전화번호: 앞자리 중복 많음
- 이름: 특정 문자 빈도 높음

 3. 주요 해시 함수 기법

(1) Division Method (나눗셈법)

java

```java
int hash(int key, int tableSize) {
    return key % tableSize;
}
```

**최적화 방법:**

- 테이블 크기를 **소수(prime number)**로 선택
- 2의 거듭제곱은 피할 것 (하위 비트만 사용되어 충돌 증가)

**좋은 예:** tableSize = 97, 101, 211 **나쁜 예:** tableSize = 100, 128, 256

(2) Multiplication Method (곱셈법)

java

```java
int hash(int key, int tableSize) {
    double A = 0.6180339887; // 황금비 (√5 - 1) / 2
    double temp = key * A;
    double fractional = temp - Math.floor(temp);
    return (int)(tableSize * fractional);
}
```

**장점:**

- 테이블 크기에 덜 민감
- A = 0.6180339887 (황금비)가 일반적으로 좋은 성능

(3) Universal Hashing

java

```java
int hash(int key, int tableSize) {
    int a = random_prime(); // 무작위 소수
    int b = random_number(); // 무작위 수
    int p = large_prime;     // 큰 소수
    
    return ((a * key + b) % p) % tableSize;
}
```

**장점:**

- 최악의 경우를 평균적으로 만듦
- 악의적인 입력에 강함

4. 문자열 해시 함수

(1) Polynomial Rolling Hash

java

```java
int hashString(String s, int tableSize) {
    int hash = 0;
    int prime = 31; // 또는 37, 53 등 작은 소수
    
    for (int i = 0; i < s.length(); i++) {
        hash = hash * prime + s.charAt(i);
    }
    
    return Math.abs(hash % tableSize);
}
```

**왜 31을 사용?**

- 작은 소수로 오버플로우 관리 용이
- 31 * x = (x << 5) - x (비트 연산으로 최적화 가능)
- Java String의 hashCode()가 사용

(2) Horner's Method 개선

java

```java
long hashString(String s, int tableSize) {
    long hash = 0;
    long prime = 31;
    long mod = 1_000_000_007; // 큰 소수
    
    for (int i = 0; i < s.length(); i++) {
        hash = (hash * prime + s.charAt(i)) % mod;
    }
    
    return (int)(hash % tableSize);
}
```
5. 복합 키 해시 함수

java

```java
class Person {
    String name;
    int age;
    String city;
    
    @Override
    public int hashCode() {
        int result = 17; // 0이 아닌 초기값
        
        // 각 필드에 다른 소수 곱하기
        result = 31 * result + (name != null ? name.hashCode() : 0);
        result = 31 * result + age;
        result = 31 * result + (city != null ? city.hashCode() : 0);
        
        return result;
    }
}
```

**원칙:**

- 0이 아닌 초기값 사용
- 각 필드마다 소수 곱하기
- null 처리

##### 해시값이 충돌했을 때, 어떤 방식으로 처리할 수 있을까요?
해시 충돌 처리 방법

해시 충돌을 해결하는 방법은 크게 두 가지로 나뉩니다.

---
 1. 체이닝 (Chaining)
개념

각 버킷을 링크드 리스트로 만들어서, 같은 해시값을 가진 데이터들을 연결해서 저장하는 방식입니다.
 동작 방식

- 충돌이 발생하면 해당 버킷의 리스트 끝에 새 노드를 추가합니다
- 검색할 때는 해당 버킷의 리스트를 순회하며 찾습니다
 장점

- 구현이 간단합니다
- 해시 테이블이 가득 차도 계속 데이터를 추가할 수 있습니다
- 삭제 연산이 간단합니다
 단점
- 추가 메모리가 필요합니다 (포인터 저장)
- 최악의 경우 O(n) 시간복잡도를 가집니다 (모든 데이터가 한 버킷에 몰릴 때)
- 캐시 효율이 낮습니다 (메모리가 연속적이지 않음)
 시간복잡도

- 평균: O(1 + α) - α는 Load Factor (n/m)
- 최악: O(n)

---
2. 개방 주소법 (Open Addressing)
 개념

충돌이 발생하면 다른 빈 버킷을 찾아서 저장하는 방식입니다. 테이블 내에서만 해결합니다.

 2-1. 선형 탐사 (Linear Probing)

**동작 방식:**

- 충돌 시 순차적으로 다음 버킷을 확인합니다
- h(k), h(k)+1, h(k)+2, h(k)+3 순서로 탐색

**장점:**

- 구현이 간단합니다
- 캐시 효율이 좋습니다

**단점:**

- Primary Clustering 문제가 발생합니다
- 특정 영역에 데이터가 몰려서 성능이 저하됩니다

**예시:**

- 5번 버킷에 충돌 → 6번 확인 → 7번 확인 → 8번 확인...

2-2. 이차 탐사 (Quadratic Probing)

**동작 방식:**

- 충돌 시 제곱수만큼 떨어진 버킷을 확인합니다
- h(k), h(k)+1², h(k)+2², h(k)+3² 순서로 탐색

**장점:**

- Primary Clustering을 어느 정도 완화합니다

**단점:**

- Secondary Clustering이 발생할 수 있습니다
- 같은 해시값을 가진 키들은 같은 탐사 순서를 따릅니다

**예시:**

- 5번 버킷에 충돌 → 6번(+1) 확인 → 9번(+4) 확인 → 14번(+9) 확인...

2-3. 이중 해싱 (Double Hashing)

**동작 방식:**

- 두 번째 해시 함수를 사용해서 탐사 간격을 결정합니다
- h(k) + i × h₂(k) 순서로 탐색

**장점:**

- Clustering 문제를 가장 잘 해결합니다
- 키마다 다른 탐사 순서를 가집니다

**단점:**

- 두 개의 해시 함수가 필요합니다
- 계산이 조금 더 복잡합니다

**예시:**

- h₁(k) = k % 13
- h₂(k) = 7 - (k % 7)
- 충돌 시 h₂(k)만큼씩 이동

개방 주소법의 공통 특징

**삭제 처리:**

- 단순히 삭제하면 탐사 체인이 끊어집니다
- "deleted" 표시를 사용하거나 재배치가 필요합니다

**Load Factor:**

- 0.7 이하로 유지하는 것이 좋습니다
- 테이블이 가득 차면 리사이징이 필수입니다

---
비교 요약

체이닝 vs 개방 주소법

**체이닝이 좋은 경우:**

- 삭제 연산이 빈번할 때
- Load Factor를 높게 유지해도 괜찮을 때
- 메모리 여유가 있을 때

**개방 주소법이 좋은 경우:**

- 메모리 효율이 중요할 때
- 캐시 성능이 중요할 때
- 데이터 크기가 작을 때

실무에서는

- Java의 HashMap은 체이닝을 사용합니다 (트리로 최적화)
- Python의 dict는 개방 주소법을 사용합니다
- C++의 unordered_map은 체이닝을 사용합니다
##### 본인이 사용하는 언어에서는, 어떤 방식으로 해시 충돌을 처리하나요?
Java의 해시 충돌 처리 방식

Java의 HashMap은 **체이닝 방식**을 기본으로 사용하되, **성능 최적화를 위해 특정 조건에서 트리 구조로 전환**하는 하이브리드 방식을 사용합니다.

---
기본 구조: Separate Chaining

Java 7 이전

- 순수한 체이닝 방식을 사용했습니다
- 각 버킷이 링크드 리스트로 구성되어 있습니다
- 충돌이 발생하면 리스트 끝에 추가합니다
문제점

- 최악의 경우 한 버킷에 모든 데이터가 몰리면 O(n) 성능이 됩니다
- 악의적인 입력으로 DoS 공격이 가능했습니다

---
Java 8 이후: 개선된 방식

핵심 개선사항

**Treeification** - 링크드 리스트를 레드-블랙 트리로 전환합니다.

전환 조건

**리스트 → 트리 전환:**

- 한 버킷의 노드 개수가 **8개 이상**이 되면
- 그리고 전체 테이블 크기가 **64 이상**일 때
- 링크드 리스트를 레드-블랙 트리로 변환합니다

**트리 → 리스트 전환:**

- 노드 개수가 **6개 이하**로 줄어들면
- 다시 링크드 리스트로 변환합니다

왜 8과 6인가?

- 8과 6 사이에 2의 차이를 두어 **히스테리시스**를 만듭니다
- 노드 개수가 7~8 사이를 오가며 빈번한 전환이 발생하는 것을 방지합니다

---
성능 개선 효과

충돌이 많을 때

**링크드 리스트:**

- 검색: O(n)
- 최악의 경우 한 버킷에 n개 노드가 있으면 n번 순회

**레드-블랙 트리:**

- 검색: O(log n)
- 균형 이진 탐색 트리이므로 log n에 탐색

실제 영향

- 악의적인 해시 충돌 공격에 대한 방어가 가능해졌습니다
- 해시 함수가 나빠서 충돌이 많이 발생해도 어느 정도 성능을 보장합니다

---
Load Factor와 리사이징

기본 설정

- 초기 용량: **16**
- Load Factor: **0.75**
- 즉, 12개(16 × 0.75)가 차면 리사이징

리사이징 동작

- 용량을 **2배**로 늘립니다
- 16 → 32 → 64 → 128 → ...
- 모든 데이터를 새로운 테이블로 재배치(rehashing)합니다

Load Factor가 0.75인 이유

- 너무 낮으면: 메모리 낭비
- 너무 높으면: 충돌 증가로 성능 저하
- 0.75는 공간과 성능의 적절한 균형점입니다

---
 해시 함수 개선
hashCode 재가공

Java는 객체의 hashCode를 그대로 사용하지 않고 한 번 더 처리합니다.

**목적:**

- 상위 비트와 하위 비트를 섞어서 분산을 개선합니다
- 특히 테이블 크기가 작을 때 하위 비트만 사용되는 문제를 완화합니다

**방식:**

- hashCode의 상위 16비트를 하위 16비트와 XOR 연산합니다
- 이렇게 하면 상위 비트 정보도 해시값에 반영됩니다

---
실제 동작 예시
정상적인 경우

```
버킷 [0]: [key1] → null
버킷 [1]: [key2] → [key3] → null  (리스트 형태)
버킷 [2]: [key4] → null
...
```
충돌이 많은 경우 (8개 이상)

```
버킷 [1]: TreeNode (레드-블랙 트리)
           /     \
       [key2]   [key5]
        /  \      /  \
    [key3][key4][key6][key7]
           ...
```

---
왜 레드-블랙 트리를 선택했나?

AVL 트리 대신 레드-블랙 트리를 사용하는 이유

**레드-블랙 트리:**

- 삽입/삭제가 더 빠릅니다 (최대 2번의 회전)
- 약간 덜 균형잡혀있지만 충분히 효율적입니다

**AVL 트리:**

- 더 균형잡혀있어 검색은 조금 더 빠릅니다
- 하지만 삽입/삭제 시 더 많은 회전이 필요합니다

HashMap에서는 검색, 삽입, 삭제가 모두 빈번하므로 **레드-블랙 트리가 더 적합**합니다.

---
정리

**Java HashMap의 충돌 처리:**

1. **기본은 체이닝** - 링크드 리스트 사용
2. **성능 최적화** - 충돌이 많으면(8개 이상) 레드-블랙 트리로 전환
3. **동적 조정** - 노드가 줄면(6개 이하) 다시 리스트로 전환
4. **Load Factor 0.75** - 용량의 75%가 차면 2배로 확장
5. **해시 재가공** - hashCode를 한 번 더 처리해서 분산 개선

이러한 설계 덕분에 Java의 HashMap은 평균 O(1), 최악의 경우에도 O(log n)의 성능을 보장하며, 악의적인 충돌 공격에도 어느 정도 방어가 가능합니다.
##### Double Hashing 의 장점과 단점에 대해서 설명하고, 단점을 어떻게 해결할 수 있을지 설명해 주세요.
장점

1. Clustering 문제 해결

**선형 탐사의 문제:**

```
h(k) = k % 10

key 12 → 버킷 [2]
key 22 → 버킷 [2] 충돌 → [3]
key 32 → 버킷 [2] 충돌 → [3] 충돌 → [4]
key 42 → 버킷 [2] 충돌 → [3] 충돌 → [4] 충돌 → [5]

결과: [2][3][4][5] 연속된 영역에 데이터 몰림 (Primary Clustering)
```

**이중 해싱의 해결:**

```
h₁(k) = k % 10
h₂(k) = 7 - (k % 7)

key 12 → 버킷 [2]
key 22 → 버킷 [2] 충돌 → h₂(22) = 7-1 = 6만큼 이동 → [8]
key 32 → 버킷 [2] 충돌 → h₂(32) = 7-4 = 3만큼 이동 → [5]
key 42 → 버킷 [2] 충돌 → h₂(42) = 7-0 = 7만큼 이동 → [9]

결과: [2][5][8][9] 분산됨!
```
2. 키마다 다른 탐사 순서

**이차 탐사의 문제 (Secondary Clustering):**

```
같은 초기 해시값을 가진 키들은 같은 탐사 순서:

key A: hash = 5 → 5, 6, 9, 14, 21...
key B: hash = 5 → 5, 6, 9, 14, 21... (동일!)
```

**이중 해싱:**

```
key A: h₁ = 5, h₂ = 3 → 5, 8, 11, 14, 17...
key B: h₁ = 5, h₂ = 7 → 5, 12, 19, 26, 33...
(완전히 다른 경로!)
```

3. 균등 분산

- 키가 테이블 전체에 골고루 분산됩니다
- 충돌이 발생해도 빠르게 빈 버킷을 찾을 수 있습니다
- 성능이 이론적 최적값에 가깝습니다

---
단점

1. 두 개의 해시 함수 필요

**계산 비용:**

```
선형 탐사: h(k) 한 번만 계산
이중 해싱: h₁(k), h₂(k) 두 번 계산

→ 약 2배의 계산 비용
```

2. 구현 복잡도

**신경 써야 할 것들:**

- 두 해시 함수의 품질 관리
- h₂(k)가 0이 되면 안 됨 (무한 루프)
- h₂(k)와 테이블 크기가 서로소여야 함

**예시 - 잘못된 설계:**

```
테이블 크기: 10
h₂(k) = 2 (항상 2씩 이동)

초기 위치가 홀수면:
1 → 3 → 5 → 7 → 9 → 1 → 3... (짝수 버킷 탐색 불가!)
```

3. 캐시 효율 저하

**선형 탐사:**

```
연속된 메모리 위치 접근
→ 캐시 히트율 높음
→ CPU 캐시 활용 좋음

버킷 [5] → [6] → [7] → [8]
메모리 상에서 인접
```

**이중 해싱:**

```
불규칙한 점프
→ 캐시 미스 발생 가능
→ 메모리 접근 패턴 예측 어려움

버킷 [5] → [12] → [3] → [17]
메모리 상에서 멀리 떨어짐
```

4. 삭제 처리 복잡

**문제:**

```
데이터 삭제 시 "deleted" 마커 필요
→ 탐사 체인이 끊어지면 안 됨
→ 시간이 지날수록 deleted 마커가 쌓임
→ 성능 저하
```

5. Load Factor 제한

**개방 주소법의 공통 문제:**

```
Load Factor가 0.7~0.8 넘으면 급격한 성능 저하
→ 체이닝보다 엄격한 제한
→ 더 자주 리사이징 필요
```

---
단점 해결 방법

 해결책 1: 해시 함수 최적화

**미리 계산된 해시 함수 사용**

```
장점:
- 계산 비용을 줄일 수 있음
- 런타임 오버헤드 감소

방법:
- 비트 연산 활용
- 룩업 테이블 사용
- 간단한 연산으로 h₂ 유도

예시:
h₁(k) = k % m
h₂(k) = 1 + (k % (m-1))  // 간단하면서도 효과적
```

해결책 2: h₂ 설계 원칙 준수

**안전한 h₂ 설계:**

```
규칙 1: h₂(k)는 절대 0이 되면 안 됨
→ h₂(k) = 1 + (k % p) 처럼 최소값 보장

규칙 2: h₂(k)와 테이블 크기가 서로소
→ 테이블 크기를 소수로 선택
→ 또는 h₂(k)가 항상 홀수가 되게 설계

좋은 예:
테이블 크기 m = 소수
h₂(k) = 1 + (k % (m-1))
→ h₂는 1 ~ m-1 범위
→ m과 항상 서로소 관계
```

해결책 3: 하이브리드 접근

**상황에 따라 방법 전환:**

```
Load Factor < 0.5:
→ 선형 탐사 사용 (캐시 효율 활용)

Load Factor ≥ 0.5:
→ 이중 해싱 사용 (충돌 감소)

장점:
- 낮은 부하에서는 캐시 효율
- 높은 부하에서는 충돌 방지
```

해결책 4: 삭제 문제 해결

**방법 A: Lazy Deletion + 주기적 재정리**

```
삭제 시:
1. "deleted" 마커만 표시
2. deleted 비율이 20% 넘으면
   → 전체 재해싱 수행
   → deleted 제거하고 재배치

장점: 간단한 구현
단점: 주기적 오버헤드
```

**방법 B: Robin Hood Hashing**

```
개념:
- 충돌 시 "더 먼 곳에서 온" 키에게 우선권
- 삭제 후 재배치로 빈 공간 최소화

예시:
A는 원래 위치에서 1칸 떨어짐
B는 원래 위치에서 5칸 떨어짐
→ B가 더 "가난"하므로 B를 우선 배치
```

**방법 C: Tombstone Recycling**

```
deleted 위치를 새 데이터 삽입 시 재활용

삽입 알고리즘:
1. 탐사하면서 첫 번째 deleted 위치 기억
2. 빈 버킷 찾으면 deleted 위치에 삽입
3. deleted 마커 정리

효과: deleted가 자연스럽게 줄어듦
```

해결책 5: Load Factor 관리

**적극적인 리사이징:**

```
보수적 임계값:
- Load Factor 0.5~0.6에서 리사이징
- 체이닝(0.75)보다 낮은 기준

점진적 리사이징:
- 한 번에 전체 재해싱하지 않고
- 여러 번의 연산에 걸쳐 점진적으로 이동
- 각 연산의 지연 시간 감소
```

해결책 6: 캐시 친화적 변형

**Hopscotch Hashing:**

```
개념:
- 각 버킷 주변 일정 범위(neighborhood) 내에서만 탐사
- 범위: 보통 32 정도

장점:
- 지역성 보장 (캐시 효율)
- 이중 해싱의 충돌 완화 효과
- 예측 가능한 메모리 접근

예시:
버킷 [10]에 저장 시도
→ [10]~[41] 범위 내에서만 탐색
→ 캐시 라인 안에 들어갈 가능성
```

```
✅ 사용하는 경우:
- 충돌이 많이 예상되는 경우
- 보안이 중요한 경우 (예측 불가능성)
- 균등 분산이 중요한 경우
- Load Factor를 높게 유지해야 할 때

❌ 피하는 경우:
- 캐시 효율이 중요한 경우
- 구현 단순성이 우선인 경우
- 데이터 크기가 매우 작은 경우
- 삭제가 빈번한 경우
```


```
선형 탐사:
+ 구현 간단
+ 캐시 효율 최고
- Primary Clustering

이차 탐사:
+ 구현 비교적 간단
+ Primary Clustering 완화
- Secondary Clustering

이중 해싱:
+ Clustering 문제 완전 해결
+ 최고의 균등 분산
- 구현 복잡
- 캐시 효율 낮음

체이닝:
+ 구현 간단
+ Load Factor 제한 없음
+ 삭제 간단
- 추가 메모리
- 포인터 오버헤드
```

---


**이중 해싱의 핵심:**

**장점 = Clustering 완벽 해결**

- 최고의 충돌 분산 성능
- 이론적 최적값에 근접

**단점 = 구현 복잡도와 캐시**

- 두 해시 함수 관리
- 캐시 효율 저하
- 삭제 처리 복잡

**해결의 핵심:**

- 좋은 h₂ 설계 (0 방지, 서로소 보장)
- 적극적 Load Factor 관리
- 하이브리드 접근 (상황별 전환)
- Hopscotch 같은 변형 사용

**실무에서는:** 대부분 체이닝을 사용합니다. 구현이 간단하고 안정적이며, 이중 해싱의 복잡도가 주는 성능 이득보다 관리 비용이 더 큽니다. 하지만 메모리가 제한적이거나 캐시 지역성보다 균등 분산이 중요한 특수한 경우에는 이중 해싱이 좋은 선택이 될 수 있습니다!
##### Load Factor에 대해 설명해 주세요. 본인이 사용하는 언어에서의 해시 자료구조는 Load Factor에 관련한 정책이 어떻게 구성되어 있나요?
Load Factor (부하 계수)

---
Load Factor란?

정의

```
Load Factor = 저장된 엔트리 수 / 테이블 크기
            = n / m

n = 현재 저장된 데이터 개수
m = 해시 테이블의 버킷 개수
```
의미

**"해시 테이블이 얼마나 차있는가"를 나타내는 비율**

---
Load Factor의 영향

Load Factor가 낮을 때 (예: 0.3)

```
테이블: 100개 버킷
데이터: 30개
Load Factor = 0.3

상태:
[0] A
[1] 비어있음
[2] 비어있음
[3] B
[4] 비어있음
[5] 비어있음
...

장점:
- 충돌 거의 없음
- 검색이 매우 빠름 (거의 O(1))
- 빈 공간 찾기 쉬움

단점:
- 메모리 낭비 (70% 공간이 빔)
```

Load Factor가 높을 때 (예: 0.9)

```
테이블: 100개 버킷
데이터: 90개
Load Factor = 0.9

상태:
[0] A
[1] B → C
[2] D → E → F
[3] G
[4] H → I
[5] J → K → L
...

장점:
- 메모리 효율적 (90% 사용)

단점:
- 충돌 많이 발생
- 체이닝의 리스트가 길어짐
- 검색 속도 저하
- 삽입/삭제 느려짐
```

---
성능과의 관계

체이닝 방식

**평균 탐색 시간:**

```
성공적인 검색: O(1 + α/2)
실패한 검색: O(1 + α)

α = Load Factor

예시:
α = 0.5 → 평균 1.25회 비교
α = 1.0 → 평균 2회 비교
α = 2.0 → 평균 3회 비교
```

개방 주소법

**Load Factor가 1에 가까울수록 급격히 성능 저하**

```
α = 0.5 → 평균 탐사 1.5회
α = 0.7 → 평균 탐사 3.3회
α = 0.9 → 평균 탐사 10회
α = 1.0 → 삽입 불가능!
```

---
Java HashMap의 Load Factor 정책

1. 기본 설정

java

````java
// 기본값
초기 용량(Initial Capacity): 16
Load Factor: 0.75
임계값(Threshold): 16 × 0.75 = 12
```

**의미:**
- 16개 버킷으로 시작
- 12개가 차면 리사이징 발동

### 2. 왜 0.75인가?

**이론적 근거:**
- 시간과 공간의 좋은 균형점
- 포아송 분포 분석 결과
- 0.75에서 충돌 확률과 메모리 효율이 최적

**실험적 결과:**
```
0.5: 메모리 낭비 심함 (50% 사용)
0.75: 균형잡힌 성능
1.0: 충돌 증가로 성능 저하
```

### 3. 리사이징 과정

**Threshold(임계값) 계산:**
```
Threshold = Capacity × Load Factor

예시:
Capacity 16, Load Factor 0.75
→ Threshold = 12

데이터가 12개 이상 들어가면 리사이징
```

**리사이징 동작:**
```
1. 현재 크기의 2배로 확장
   16 → 32 → 64 → 128 → 256 ...

2. 새 테이블 생성

3. 모든 데이터 재해싱(Rehashing)
   - 테이블 크기가 바뀌었으므로
   - hash(key) % 새크기
   - 새로운 버킷에 재배치

4. 새 Threshold 계산
   32 × 0.75 = 24
````

---
Java HashMap 생성자

1. 기본 생성자

java

```java
HashMap<String, Integer> map = new HashMap<>();

// 내부 설정
capacity = 16
loadFactor = 0.75
threshold = 12
```

2. 초기 용량 지정

java

````java
HashMap<String, Integer> map = new HashMap<>(32);

// 내부 설정
capacity = 32
loadFactor = 0.75 (기본값)
threshold = 24
```

**주의사항:**
```
실제로 100개 데이터를 저장할 계획이라면?

나쁜 예:
new HashMap<>(100)
→ 100 × 0.75 = 75에서 리사이징
→ 100개 넣기 전에 리사이징 발생!

좋은 예:
new HashMap<>(100 / 0.75 + 1)
= new HashMap<>(134)
→ 134 × 0.75 = 100.5
→ 100개까지 리사이징 없음
````

3. Load Factor까지 지정

java

````java
HashMap<String, Integer> map = new HashMap<>(32, 0.5f);

// 내부 설정
capacity = 32
loadFactor = 0.5
threshold = 16
```

**언제 사용?**

**Load Factor 낮게 (0.5):**
```
- 검색 속도가 매우 중요할 때
- 메모리 여유가 있을 때
- 충돌을 최소화하고 싶을 때

예: 실시간 검색 시스템
```

**Load Factor 높게 (0.9):**
```
- 메모리가 제한적일 때
- 삽입/삭제보다 저장 공간이 중요할 때

예: 모바일 앱, 임베디드 시스템
````

---
구체적인 동작 예시

시나리오: HashMap 사용

java

````java
HashMap<String, Integer> map = new HashMap<>();
// capacity = 16, loadFactor = 0.75, threshold = 12

// 1~12개 삽입
map.put("A", 1);
map.put("B", 2);
...
map.put("L", 12);
// 상태: size=12, capacity=16
// 아직 리사이징 안 함

// 13번째 삽입
map.put("M", 13);
// size=13 > threshold(12)
// 리사이징 발동!

리사이징 과정:
1. 새 테이블 생성: capacity = 32
2. 12개 데이터 모두 재해싱
3. 새로운 위치에 재배치
4. threshold = 32 × 0.75 = 24
5. "M" 삽입

// 14~24개 삽입
map.put("N", 14);
...
map.put("X", 24);
// 상태: size=24, capacity=32
// 아직 리사이징 안 함

// 25번째 삽입
map.put("Y", 25);
// size=25 > threshold(24)
// 다시 리사이징!
// capacity = 64
```

---

## Load Factor와 성능 측정

### 실험 결과
```
데이터 1000개 삽입 시:

Load Factor 0.5:
- 메모리: 2048 버킷
- 평균 체이닝 길이: 0.5
- 검색 시간: 매우 빠름
- 리사이징 횟수: 많음

Load Factor 0.75 (기본):
- 메모리: 2048 버킷
- 평균 체이닝 길이: 0.75
- 검색 시간: 빠름
- 리사이징 횟수: 적당
- ★ 최적의 균형

Load Factor 1.0:
- 메모리: 1024 버킷
- 평균 체이닝 길이: 1.0
- 검색 시간: 보통
- 리사이징 횟수: 적음
- 충돌 증가 시작
````

---
다른 Java 자료구조의 Load Factor

HashSet

java

```java
// HashMap을 내부적으로 사용
HashSet<String> set = new HashSet<>();

// 동일한 정책
capacity = 16
loadFactor = 0.75
```

ConcurrentHashMap

java

````java
ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();

// 동일한 기본값
capacity = 16
loadFactor = 0.75

// 추가 특징
- 세그먼트별로 독립적인 Load Factor
- 동시성 제어
```

---

## Load Factor 선택 가이드

### Load Factor 0.5 선택
```
✅ 사용하는 경우:
- 검색이 매우 빈번
- 메모리 충분
- 실시간 성능 중요
- 데이터 크기가 예측 가능

예: 캐시 시스템, 실시간 검색
```

### Load Factor 0.75 선택 (권장)
```
✅ 사용하는 경우:
- 대부분의 일반적인 경우
- 균형잡힌 성능 필요
- 표준 웹 애플리케이션
- 특별한 이유가 없을 때

예: 대부분의 애플리케이션
```

### Load Factor 1.0 선택
```
✅ 사용하는 경우:
- 메모리가 제한적
- 삽입/삭제가 적음
- 데이터가 거의 고정
- 읽기 전용에 가까움

예: 설정 파일, 정적 데이터 저장
````

---
주의사항

1. 초기 용량 설정의 중요성

java

```java
// 나쁜 예: 리사이징 여러 번 발생
HashMap<String, Integer> map = new HashMap<>();
for (int i = 0; i < 10000; i++) {
    map.put("key" + i, i);
}
// 리사이징: 16→32→64→128→256→512→1024→2048→4096→8192

// 좋은 예: 리사이징 최소화
int expectedSize = 10000;
int capacity = (int) (expectedSize / 0.75 + 1);
HashMap<String, Integer> map = new HashMap<>(capacity);
// 초기부터 충분한 크기
// 리사이징 0~1회
```

2. Load Factor 변경은 신중하게

java

````java
// 기본값이 보통 최선
HashMap<String, Integer> map = new HashMap<>();

// 특별한 이유 없이 변경하지 말 것
HashMap<String, Integer> map = new HashMap<>(16, 0.9f);
// 충돌 증가, 성능 저하 가능
```

---

## 정리

**Java HashMap의 Load Factor 정책:**

1. **기본값 0.75** - 시간과 공간의 최적 균형
2. **초기 용량 16** - 작은 크기로 시작
3. **2배씩 확장** - 16→32→64→128...
4. **Threshold = Capacity × Load Factor**
5. **size > threshold 시 리사이징**

**핵심 원칙:**
- 대부분 기본값(0.75) 사용
- 예상 데이터 크기를 알면 초기 용량 지정
- 메모리와 성능 중 우선순위에 따라 조정
- 리사이징은 비용이 크므로 최소화

**공식:**
```
적절한 초기 용량 = (예상 데이터 수 / Load Factor) + 1
````
##### 다른 자료구조와 비교하여, 해시 테이블은 멀티스레드 환경에서 심각한 수준의 Race Condition 문제에 빠질 위험이 있습니다. 성능 감소를 최소화 한 채로 해당 문제를 해결할 수 있는 방법을 설계해 보세요.
1. 왜 해시 테이블은 멀티스레드에서 특히 위험한가

해시 테이블은 구조적으로 **공유 상태(shared mutable state)** 가 많습니다.

핵심 위험 포인트

1. **버킷 배열**
    
    - 서로 다른 key라도 같은 bucket으로 해싱될 수 있음
        
2. **충돌 처리 구조**
    
    - linked list / tree 구조 → 중간 삽입·삭제 시 구조 깨짐
        
3. **리사이징**
    
    - rehash 과정에서 전체 테이블 구조 변경
        
    - 동시 접근 시 거의 재앙
        

➡️ 결과적으로

> _읽기/쓰기 모두에서 Race Condition 발생 가능_  
> _심하면 무한 루프, 데이터 유실, JVM crash 수준까지 발생_

---

2. 가장 단순한 해결책과 그 한계

2.1 전체 테이블에 하나의 락

`synchronized put() synchronized get()`

**문제점**

- 모든 연산 직렬화
    
- read-heavy 환경에서도 성능 폭락
    
- 멀티스레드 쓰는 의미가 사라짐
    

➡️ **Correct하지만 쓸 수 없는 설계**

---
3. 성능을 지키는 핵심 아이디어

해결 방향은 딱 세 가지 키워드로 요약됩니다.

> **Lock 분할 + Read/Write 분리 + 구조 변경 최소화**

이걸 어떻게 구현하느냐가 설계의 핵심입니다.

---
4. 설계안 1: Lock Striping (버킷 단위 락 분할)

개념

- 해시 테이블 전체가 아니라 **여러 개의 락으로 쪼갬**
    
- key → hash → 특정 락만 획득
    

`Lock[0] ─ bucket 0,4,8 Lock[1] ─ bucket 1,5,9 Lock[2] ─ bucket 2,6,10 ...`

장점

- 서로 다른 bucket 접근은 병렬 수행 가능
    
- 락 충돌 확률 급감
    
- 구현 난이도 낮음
    

단점

- 같은 락에 매핑된 bucket은 여전히 경쟁
    
- resize 시 여전히 조심 필요
    

➡️ **JDK 7의 ConcurrentHashMap 구조**

---
5. 설계안 2: Read-Write Lock + 불변 읽기

핵심 아이디어

- `get()`은 최대한 락 없이 혹은 read lock만
    
- `put()/remove()`만 write lock
    
적용 방식

- bucket 단위로 `ReentrantReadWriteLock`
    
- read-heavy 환경에서 효과 극대화
    
주의점

- write starvation 위험
    
- read lock 자체도 비용은 있음
    

➡️ **읽기 비중이 압도적으로 높은 캐시 계열에 적합**

---
6. 설계안 3: CAS 기반 Lock-Free 접근 (고급)

핵심 아이디어

- 락 대신 **원자적 연산(CAS)** 사용
    
- bucket entry 교체를 통째로 수행
    

`Node old = table[i] Node new = new Node(key, value, old) CAS(table[i], old, new)`

장점

- 락 없음 → 스레드 컨텍스트 스위칭 감소
    
- 높은 처리량
    

단점

- 구현 난이도 매우 높음
    
- ABA 문제, 메모리 가시성 고려 필요
    

➡️ **JDK 8+ ConcurrentHashMap이 이 방식 사용**

---
7. 리사이징 문제는 어떻게 해결하나?

리사이징은 해시 테이블 Race Condition의 끝판왕입니다.

전략

1. **lazy resize**
    
    - 접근 시점에 조금씩 옮김
        
2. **bucket 단위 재해싱**
    
    - 전체 중단 X
        
3. **이중 테이블 유지**
    
    - old → new 점진적 이전
        

➡️ “resize는 한 번에 하지 않는다”가 핵심

---
8. 최종 설계 요약 (면접용 정리)

> 해시 테이블은 버킷, 충돌 구조, 리사이징으로 인해 멀티스레드 환경에서 Race Condition에 매우 취약합니다.  
> 이를 해결하기 위해 전체 락 대신 **Lock Striping으로 락을 분할**하고, **읽기/쓰기 경로를 분리**하며, 가능하다면 **CAS 기반의 lock-free 업데이트**를 사용합니다.  
> 또한 리사이징은 점진적으로 수행하여 구조 변경으로 인한 경쟁을 최소화합니다.  
> 이러한 설계를 통해 정합성을 보장하면서도 멀티스레드 환경에서 높은 처리량을 유지할 수 있습니다.

---

### 5. 트리와 이진트리, 이진탐색트리에 대해 설명해 주세요.
##### 설명
트리(Tree)는 노드들이 계층적으로 연결된 비선형 자료구조로 나무를 뒤집은 형태입니다.

[**이진 트리(Binary Tree)**](https://www.google.com/search?q=%EC%9D%B4%EC%A7%84+%ED%8A%B8%EB%A6%AC%28Binary+Tree%29&oq=%ED%8A%B8%EB%A6%AC%EC%99%80+%EC%9D%B4%EC%A7%84%ED%8A%B8%EB%A6%AC%2C+%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89%ED%8A%B8%EB%A6%AC%EC%97%90+%EB%8C%80%ED%95%B4+%EC%84%A4%EB%AA%85%ED%95%B4+%EC%A3%BC%EC%84%B8%EC%9A%94.&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQABjvBTIHCAIQABjvBTIHCAMQABjvBdIBBzQzM2owajSoAgCwAgE&sourceid=chrome&ie=UTF-8&sei=aEeLac2BGbrj2roPyb3VyQc&ved=2ahUKEwivvoTTl8-SAxWhr1YBHd0CMdQQgK4QegQIARAB)는 모든 노드가 최대 2개의 자식만 갖는 트리이며, [**이진 탐색 트리(Binary Search Tree, BST)**](https://www.google.com/search?q=%EC%9D%B4%EC%A7%84+%ED%83%90%EC%83%89+%ED%8A%B8%EB%A6%AC%28Binary+Search+Tree%2C+BST%29&oq=%ED%8A%B8%EB%A6%AC%EC%99%80+%EC%9D%B4%EC%A7%84%ED%8A%B8%EB%A6%AC%2C+%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89%ED%8A%B8%EB%A6%AC%EC%97%90+%EB%8C%80%ED%95%B4+%EC%84%A4%EB%AA%85%ED%95%B4+%EC%A3%BC%EC%84%B8%EC%9A%94.&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQABjvBTIHCAIQABjvBTIHCAMQABjvBdIBBzQzM2owajSoAgCwAgE&sourceid=chrome&ie=UTF-8&sei=aEeLac2BGbrj2roPyb3VyQc&ved=2ahUKEwivvoTTl8-SAxWhr1YBHd0CMdQQgK4QegQIARAC)는 왼쪽 자식은 부모보다 작고 오른쪽은 큰 규칙을 추가하여 효율적인 탐색(평균

![](data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==)

O(logn)을 가능하게 하는 트리입니다.

##### 그래프와 트리의 차이가 무엇인가요?
순환 x
##### 이진탐색트리에서 중위 탐색을 하게 되면, 그 결과는 어떤 의미를 가지나요?
수가 정렬
##### 이진탐색트리의 주요 연산에 대한 시간복잡도를 설명하고, 왜 그런 시간복잡도가 도출되는지 설명해 주세요.
insert: n
delete: n
search: n
![[Pasted image 20260211000530.png]]

##### 이진탐색트리의 한계점에 대해 설명해주세요.

##### 이진탐색트리의 값 삽입, 삭제 방법에 대해 설명하고, 어떤식으로 값을 삽입하면 편향이 발생할까요?
삭제하려는 노드(4번)이 왼쪽, 오른쪽 둘 방향 모두 서브트리를 가지고 있으면 삭제하려는 노드의 후계노드(5번)을 찾아서 후계노드의 값을 복사 받고, 후계노드와 부모노드 사이의 연결을 해제한 후 부모노드는 후계노드의 서브트리를 가르키게 (연결)한다. 

후계노드를 찾는 방법은 삭제하려는 노드의 왼쪽 서브트리 중에서 제일 큰값을 고르거나 오른쪽 서브트리에서 제일 작은 값을 고른다. 이진탐색트리의 성질 (왼쪽 서브트리 < 루트(부모) < 오른쪽 서브트리) 때문에 삭제 연산이 자연스럽게 구현된다.

![[Pasted image 20260211000922.png]]
##### 이진탐색트리와 동일한 로직을 사용하면, 삼진탐색트리도 정의할 수 있을까요? 안 된다면, 그 이유에 대해 설명해 주세요.
![[Pasted image 20260211002229.png]]


### 6. 힙에 대해 설명해 주세요.
##### 설명
![[Pasted image 20260211002640.png]]

##### 힙을 배열로 구현한다고 가정하면, 어떻게 값을 저장할 수 있을까요?
완전 이진트리니까 index사용해서 값 저장 가능 루트로 갈때마다 /2

##### 힙의 삽입, 삭제 방식에 대해 설명하고, 왜 이진탐색트리와 달리 편향이 발생하지 않는지 설명해 주세요.
![[Pasted image 20260211003023.png]]

![[Pasted image 20260211003210.png]]

##### 힙 정렬의 시간복잡도는 어떻게 되나요? Stable 한가요?
힙 정렬(Heap Sort)은 최선, 평균, 최악의 경우 모두

O(nlogn)
동일한 값을 가진 요소들의 상대적인 순서가 뒤바뀔 수 있어 ==**Stable(안정) 정렬이 아닙니다**==.

### 7. BBST (Balanced Binary Search Tree) 와, 그 종류에 대해 설명해 주세요.
##### 설명
- `균형 이진 탐색 트리`는 노드의 삽입과 삭제가 발생할 때 `균형`을 유지하도록 하는 트리
- 특정 노드에 대해 최악의 경우에도 탐색, 삽입, 삭제 연산이 `O(logn)`의 시간 복잡도를 가지도록 설계된 **이진 탐색 트리**
- `균형 이진 탐색 트리`는 `이진 탐색 트리`의 한 종류
- **AVL 트리 , 레드-블랙트리(Red-Black Tree) , B 트리, B+ 트리, B* 트리**,2-3-4 Tree
##### Red Black Tree는 어떻게 균형을 유지할 수 있을까요?
1.삽입 노드 항상 red -> 삽입 후에도 5번 규칙을 만족하기 위해
![[Pasted image 20260211011737.png]]

![[Pasted image 20260211011942.png]]
##### Red Black Tree의 주요 성질 4가지에 대해 설명해 주세요.
1. 모든 노드는 빨간색 혹은 **검은색이다.**  
2. 루트 노드는 **검은색이다.**  
3. 모든 리프 노드(NIL)들은 **검은색이다.** (NIL : null leaf, 자료를 갖지 않고 트리의 끝을 나타내는 노드)  
4. 빨간색 노드의 자식은 **검은색이다.**  (삽입시 자주 위반)
   == No Double Red(빨간색 노드가 연속으로 나올 수 없다)  
5. 임의의 노드에서 nil노드 까지 가는 경로들의 black 수는 같다.  (삽입시 자주 위반)
   == 리프노드에서 루트 노드까지 가는 경로에서 만나는 **검은색** 노드의 개수가 같다.

##### 2-3-4 Tree, AVL Tree 등의 다른 BBST 가 있음에도, 왜 Red Black Tree가 많이 사용될까요?
1. BBST들의 “균형 강도”부터 비교

AVL Tree

- 매우 엄격한 균형
    
- 높이 ≈ `1.44 log N`
    
- **탐색 성능 최강**
    

Red-Black Tree

- 느슨한 균형
    
- 높이 ≤ `2 log N`
    
- AVL보다 조금 더 깊음
    

2-3-4 Tree

- 완전 균형
    
- 이론적으로 가장 안정적인 높이
    

👉 **탐색 성능만 보면**

`AVL > RBT`

그런데 실무에서는 RBT가 압도적으로 많이 쓰입니다. 이유는 “탐색”이 아니라 **“갱신 비용”**에 있습니다.

---
2. 실무에서 더 중요한 건 “회전 횟수”

AVL Tree의 문제

- 삽입/삭제 후 **balance factor 전파**
    
- **연쇄적인 회전 발생 가능**
    
- 삭제는 특히 비쌈
    

➡️ 쓰기 빈도가 높을수록 비용 증가

---
Red-Black Tree의 장점

- 색깔 규칙만 유지하면 됨
    
- **회전 횟수가 매우 적음**
    
- 대부분
    
    - 삽입: 회전 1~2회
        
    - 삭제: 색 변경 위주
        

➡️ **쓰기 성능이 훨씬 안정적**

---
3. 캐시·브랜치 관점에서의 실용성

AVL

- 엄격한 균형 → 구조 변경 빈번
    
- 회전 많음 → cache miss 증가
    
 RBT

- 구조 변경 최소화
    
- 색 변경은 포인터 이동 없음
    

➡️ **현대 CPU에서 체감 성능은 RBT가 더 좋을 수 있음**

---
4. 2-3-4 Tree는 왜 안 쓰일까?

이론적 장점

- 항상 완전 균형
    
- 탐색 경로 짧음
    
현실적 문제

- 노드 구조 복잡
    
- 분기/병합 로직 어려움
    
- 메모리 레이아웃 비효율
    

➡️ **대신**

- 2-3-4 Tree를 **이진 트리로 표현한 형태**가 바로 Red-Black Tree
    

> 🔑 RBT는 2-3-4 Tree의 이진 표현(binary encoding)

그래서

- 같은 균형 보장
    
- 구현 난이도 낮음
    
- 포인터 구조 단순
    

---
5. 삭제 연산에서의 결정적 차이
AVL 삭제

- balance factor 계속 확인
    
- 상위 노드까지 재조정
    
RBT 삭제

- “double black” 개념
    
- 색 이동 중심
    
- 회전 최소
    

➡️ 삭제 많은 자료구조(Map, Set)에 치명적인 차이

---
6. 표준 라이브러리 선택의 이유

실제로도 이렇게 선택되었습니다.

- Java `TreeMap`, `TreeSet` → **Red-Black Tree**
    
- C++ `map`, `set` → **Red-Black Tree**
    
- Linux kernel RB tree
    
- BSD, LLVM 등 다수 시스템
    

이유 요약:

> 구현 안정성 + 예측 가능한 성능 + 유지보수 용이성

### 8. 정렬 알고리즘에 대해 설명해 주세요.
##### 설명
![[Pasted image 20260211012300.png]]

![[Pasted image 20260211012422.png]]
##### Quick Sort와 Merge Sort를 비교해 주세요.
https://hongjw1938.tistory.com/192
![[Pasted image 20260211014047.png]]
##### Quick Sort에서 O(N^2)이 걸리는 예시를 들고, 이를 개선할 수 있는 방법에 대해 설명해 주세요.
https://hongjw1938.tistory.com/31
**Pivot 결정하기**

아래와 같은 배열을 정렬해야 한다면 어떨까?

![](https://blog.kakaocdn.net/dna/cxDOWG/btqNf8re2VE/AAAAAAAAAAAAAAAAAAAAAHJu_EeDt73TXjPsMzEbyu6-1iknhN_hkN6ZHYo8HwdT/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=UH8V8S8CHrV1a6kNsFK1SM%2BCT1Q%3D)

Pivot을 가장 좌측(첫 번째 값)으로 지정했고 오름차순 정렬을 하려는데 뒤의 값이 내림차순 기준으로 되어 있으며 41은 이미 가장 큰 숫자라고 해보자. 그럼 1차 정렬을 완료하면 아래와 같은 결과가 된다.

![](https://blog.kakaocdn.net/dna/qlBpM/btqNfrEBVZy/AAAAAAAAAAAAAAAAAAAAAN4bU0h_Cf8oNbNkQqhFdkTrs2b0GH1IjGMSUWB_M_Bq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=G9%2FQwzd3ROfpwFxQRKCI%2FBsdbK8%3D)

1차 정렬 완료 후

즉, Pivot의 위치를 찾았더니 모든 값과 비교하여 Swap이 되었을 것이고 가장 오른쪽에 위치하게 되었을 것이다.

Pivot이 적당히 중간 즈음 위치했다면 양 쪽의 부분 집합이 균등하게 배분되어 전체 비교 횟수를 현저히 줄일 수 있는데 이와 같은 현상이 반복된다면 비교를 더욱 많이 해야 한다. 사실상 **버블 정렬과 다를 바가 없다.**

이런 최악의 경우, 전체 시간 복잡도는 **O(n^2)** 만큼 발생하게 되어 퀵 정렬의 장점이 사라지게 된다.

그래서, 이와 같이 무조건 최 우측/좌측의 값을 Pivot으로 무조건 설정하는 것은 좋은 방법이 아니다. 최악의 성능을 회피하기 위해 아래와 같은 방법을 예시로 사용할 수 있다.

**① 그냥 Random하게 골라보기**

랜덤하게 설정하면 항상 최악의 Case가 되지는 않을 것이다. 그러나 최악의 Case가 생겨날 수는 있다. (평균적으로 O(nlogn) 유지)

**② Median-Of-3 method**

이는 배열의 값들 중 가장 좌측 / 우측, 중앙값 3개를 뽑고, 그 3개의 값을 우선적으로 정렬한 뒤, 그 중 중간 값을 가장 좌측 또는 우측으로 이동시켜 Pivot으로 설정한 뒤 퀵 정렬을 수행하는 방법이다.

이 값이 Pivot으로 사용되어 전체 배열을 균등하게 분할할 것이라는 보장은 없지만, 최소한 이 값이 전체 값 중 최대 / 최소값에는 해당하지 않기 때문에 중심 극한 정리에 따라 **평균적으로 O(nlogn)의 시간복잡도를 유지**할 수 있게 된다.

예를 들어서 상단의 그림으로 설명한 배열을 기준으로 보면 다음과 같다.

|   |   |   |   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|---|---|---|
|**0**|1|2|3|**4**|5|6|7|8|**9**|
|**35**|33|42|10|**14**|19|27|44|26|**31**|

여기서 First 는 35, Mid 는 14, Last는 31인데 정렬하면 **Last인 31이 Median 값**이 되고 **이 값을 Pivot으로 사용**하면 되는 것이다. 하필 이번에는 기존의 경우와 동일한데 개념만 이해하면 되겠다.

##### Stable Sort가 무엇이고, 어떤 정렬 알고리즘이 Stable 한지 설명해 주세요.
![[Pasted image 20260211012808.png]]
##### Merge Sort를 재귀를 사용하지 않고 구현할 수 있을까요?
가능

네, **가능합니다.**  
머지소트는 **재귀 없이도** 구현할 수 있고, 이를 보통 **Bottom-Up Merge Sort(반복적 머지소트)**라고 부릅니다.

---
1. 아이디어 핵심 (Top-Down vs Bottom-Up)

기존 재귀 머지소트 (Top-Down)

1. 배열을 계속 반으로 분할 (재귀)
    
2. 가장 작은 단위부터 병합
    
재귀 없는 머지소트 (Bottom-Up)

1. **크기 1짜리 배열은 이미 정렬되어 있다고 가정**
    
2. 크기 1 → 2 → 4 → 8 … 식으로 **점점 큰 블록을 병합**
    
3. 반복문으로만 처리
    

👉 분할을 재귀로 하지 않고, **병합만 반복문으로 수행**합니다.

---
2. 동작 예시

예: `[5, 3, 8, 4, 2]`

1. size = 1  
    `[5] [3] [8] [4] [2]`
    
2. size = 2  
    `[3,5] [4,8] [2]`
    
3. size = 4  
    `[3,4,5,8] [2]`
    
4. size = 8  
    `[2,3,4,5,8]`
##### Radix Sort에 대해 설명해 주세요.
https://hongjw1938.tistory.com/33?category=884192

이전의 버블 / 선택 / 삽입 / 병합 / 퀵 정렬 등은 모두 값을 비교함으로써 정렬을 수행하게 되고 이는 아무리 좋은 성능을 낼지라도 평균적으로 O(nlogn)보다 더 빠르게 수행할 수는 없다.

그런데, 이전의 **Counting Sort(계수 정렬)**과 같이 데이터의 특성을 잘 이용하면 **거의 선형 시간에 가깝게 정렬을 수행**할 수 있었다.

하지만, Counting Sort(계수 정렬)은 모든 상황에 사용할 수는 없다. 만약 데이터의 크기와 데이터 값이 너무 차이가 심하다면 오히려 더욱 성능이 안좋아진다.

예를 들어 길이는 n인데 데이터의 크기는 0부터 n^2 까지 있다면 결국 O(n^2)의 성능으로 동작하게 되기 때문이다.

그럼, 그런 데이터 배열이 있을 때, 어떻게 선형 시간 복잡도 수준으로 해결할 수 있을까? 바로 기수 정렬이 그 답이 될 수 있다.

기수 정렬을 이해해보기 위한 그림은 다음과 같다.
![[Pasted image 20260211015239.png]]

기수 정렬의 개념은 각 데이터 값의 자리수와 자리수의 값을 이용해 반복적으로 Counting Sort를 수행하여 전체 배열을 정렬하는 방식이다.

1의 자리가 아무리 큰 숫자라도 10의 자리가 더 큰 숫자가 더 큰 숫자가 된다. 그래서 전체 자리수에는 가장 덜 중요한 자리수와 중요한 자리수가 나뉘어진다.

**LSD : Least Significant Digit으로 숫자로는 1의 자리수를 의미한다.  
MSD : Most Significant Digit으로 숫자로는 자리수가 최대 d라면 d의 자리수를 의미한다.(예, 5000이면 천의 자리수)**

어느 자리수를 먼저 기준으로 정렬하느냐에 따라 **LSD 방식, MSD 방식**으로 나뉜다.

LSD 방식은 구현에 있어서 더욱 편의적이어서 LSD 방식으로 실제로는 많이 구현한다. 그러나 MSD 방식은 꼭 모든 자리수에 대해 다 검증하지 않아도 된다는 장점이 있다.

기수 정렬의 특징은 다음과 같다.

|                       |                                         |
| --------------------- | --------------------------------------- |
| **특성**                | **설명**                                  |
| **Not In-place 알고리즘** | 배열을 나누는 과정에서 나누어진 배열을 별도로 저장할 공간이 필요하다. |
| **Stable 알고리즘**       | 같은 값의 데이터의 기존 순서 유지를 보장할 수 있다.          |
|                       |                                         |

기수 정렬의 시간  / 공간 복잡도는 아래와 같다.

|   |   |   |   |   |
|---|---|---|---|---|
|**알고리즘**|**시간복잡도(최상)**|**시간복잡도(평균)**|**시간복잡도(최악)**|**공간복잡도**|
|**기수 정렬**|O(nw)|O(nw)|O(nw)|O(n)|

시간복잡도 부분에서 w는 기수의 크기이다. 즉, 몇의 자리수까지 있느냐를 의미하여 거의 상수이기 때문에 실질적으로 O(n) 수준의 성능을 보여준다.

Counting Sort(계수 정렬)을 이해했다면 기수 정렬은 이해하기 쉽다. 코드로 이해해보자.
##### Bubble, Selection, Insertion Sort의 속도를 비교해 주세요.
https://hongjw1938.tistory.com/28

**1. Bubble Sort(버블 정렬)**

설명보단 보는 것이 제일 빠르다. 다음의 애니메이션을 보고 버블 정렬이 무엇인지 이해해보자.

![](https://blog.kakaocdn.net/dna/bm506H/btqMMhIIWDh/AAAAAAAAAAAAAAAAAAAAACQ_5uYLrjt2j3qjZSdcf68X0LNSdXaHf1OJeHJu0Zkq/img.gif?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=PlvW7U9J7WrzsmxPnU2kpVs6uhM%3D)

버블 정렬 예시, 출처 : https://java2blog.com/bubble-sort-c/

한 눈에 보아도 이해가 쉽다. 오름차순 정렬을 기준으로 보여주는 애니메이션이다. 연속된 값들을 서로 비교하면서 더 작은 값이 왼쪽에 위치하도록 상호 위치 변경을 해주는 작업을 해주고 있다.

버블 정렬의 특징은 다음과 같다.

|   |   |
|---|---|
|**특성**|**설명**|
|In-place 알고리즘|Memory 상에서 필요 시 상호 위치만 변경될 뿐 추가적인 배열 생성이 불 필요함|
|Stable 알고리즘|구현하기 나름이나, 기본적으로 값이 같으면 정렬 순서를 바꾸지 않는다.|

버블 정렬의 시간  / 공간 복잡도는 아래와 같다.

|   |   |   |   |   |
|---|---|---|---|---|
|**알고리즘**|**시간복잡도(최상)**|**시간복잡도(평균)**|**시간복잡도(최악)**|**공간복잡도**|
|버블 정렬|O(n)|O(n^2)|O(n^2)|O(1)|

**2. Selection Sort(선택 정렬)**

이번에도 애니메이션을 한 번 보자.

![[Pasted image 20260211015541.png]]

이것도 쉽게 이해할 수 있을 것이다. 전체 값을 한 번 훑고 가장 작은 값을 찾아서 현재 정렬되지 않은 partition의 가장 작은 인덱스 값과 상호 위치를 바꾸고 있다.

선택 정렬의 특징은 다음과 같다.

|   |   |
|---|---|
|**특성**|**설명**|
|In-place 알고리즘|Memory 상에서 필요 시 상호 위치만 변경될 뿐 추가적인 배열 생성이 불 필요함|
|Unstable 알고리즘|Stable을 보장할 수 없다. 그 이유는 아래에 설명|

왜 선택 정렬이 Stable을 보장할 수 없을까? 아래의 예시를 보자.

|   |   |   |   |   |   |
|---|---|---|---|---|---|
|**Index**|**0**|**1**|**2**|**3**|**4**|
|Value|4|2|3|4|1|

여기서 가장 작은 값은 1이다. 그 1의 위치는 Index 0의 위치와 상호 변경될 것이다. 그러면 **Stable을 유지하기 위해서는 Index 0의 4와 Index 3의 4가 순서가 유지**되어야 하는데 **Index3의 4가 더 앞 쪽에 자리잡게 되었기 때문에 Unstable**하다.

만약 Stable을 유지하고 싶다면 방법은, 최소값인 1을 찾고 1과 Index 0의 4를 상호 변경하는게 아니라 Index 0 ~ 3까지를 Index 1 ~ 4로 한 칸씩 미루고 Index 0 에 1을 삽입한다면 가능하다.

선택정렬의 시간 / 공간복잡도는 아래와 같다.

|   |   |   |   |   |
|---|---|---|---|---|
|**알고리즘**|**시간복잡도(최상)**|**시간복잡도(평균)**|**시간복잡도(최악)**|**공간복잡도**|
|선택 정렬|O(n^2)|O(n^2)|O(n^2)|O(1)|

선택 정렬은 최상의 경우에도 O(n^2) 수준이기는 한데, 일반적으로 버블 정렬보다 좋은 결과를 가져오기는 한다.

선택 정렬 코드는 아래와 같다. 이번에도 애니메이션과는 달리 최대값을 찾아서 끝으로 옮김(반대 방식은 직접 만들어보길 권장)

**3. Insertion sort(삽입 정렬)**

이번에도 애니메이션부터 보도록 하자.

![](https://blog.kakaocdn.net/dna/bJCFft/btqMMhvgHSK/AAAAAAAAAAAAAAAAAAAAAIPafhuqDA7Gg5O7senk7eIhF1FXE6VQ3NcfNjdAjWj8/img.gif?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=We8L56QFninF7%2FPGqrLnIpIDZwg%3D)



정렬되지 않은 부분의 인덱스 위치의 값을 그 이전에 정렬된 부분의 인덱스 내에 있는 값들과 비교하여 적절한 위치를 찾을 때까지 정렬된 부분의 값들을 한 칸씩 이동시키고 빈 자리를 찾아 삽입시키는 과정으로 진행된다.

삽입 정렬의 특징은 다음과 같다.

|   |   |
|---|---|
|**특성**|**설명**|
|In-place 알고리즘|Memory 상에서 필요 시 상호 위치만 변경될 뿐 추가적인 배열 생성이 불 필요함|
|Stable 알고리즘|구현하기 나름이나, 기본적으로 값이 같으면 정렬 순서를 바꾸지 않는다.|

삽입 정렬의 시간  / 공간 복잡도는 아래와 같다.

|   |   |   |   |   |
|---|---|---|---|---|
|**알고리즘**|**시간복잡도(최상)**|**시간복잡도(평균)**|**시간복잡도(최악)**|**공간복잡도**|
|버블 정렬|O(n)|O(n^2)|O(n^2)|O(1)|
##### 값이 **거의** 정렬되어 있거나, 아예 정렬되어 있다면, 위 세 알고리즘의 성능 비교 결과는 달라질까요?


##### 본인이 사용하고 있는 언어에선, 어떤 정렬 알고리즘을 사용하여 정렬 함수를 제공하고 있을까요?
Java는 **데이터 타입과 사용 목적에 따라 서로 다른 정렬 알고리즘**을 사용합니다. 단순히 하나의 알고리즘만 쓰지 않고, **실무 성능·안정성·메모리 특성**을 고려해 선택되어 있습니다.

---
1. `Arrays.sort()` – 기본 타입(primitive)
 대상

`int[], long[], double[], char[] ...`

사용 알고리즘

👉 **Dual-Pivot QuickSort**

특징

- 퀵소트 계열 (pivot을 2개 사용)
    
- **in-place 정렬**
    
- 평균 시간복잡도: `O(N log N)`
    
- 최악: `O(N²)` (하지만 실제로 거의 발생하지 않도록 튜닝됨)
    
- **안정 정렬 아님**
    
왜 퀵소트인가?

- primitive 타입은 **객체 비교 비용이 없음**
    
- 추가 메모리를 거의 쓰지 않음
    
- **캐시 지역성 우수**
    
- 실측 성능이 머지소트보다 빠른 경우가 많음
    

> 그래서 `Arrays.sort(int[])`는 성능 최우선 선택

---
2. `Arrays.sort()` – 객체(Object)

대상

`Integer[], String[], CustomObject[]`

사용 알고리즘

👉 **TimSort**

TimSort란?

- **Merge Sort + Insertion Sort**
    
- 실제 데이터가 **부분적으로 정렬되어 있다는 가정**을 적극 활용
    
- Python, Java에서 모두 사용
    

특징

- **안정 정렬**
    
- 최악 시간복잡도: `O(N log N)`
    
- 이미 정렬된 경우: `O(N)`
    
- 추가 메모리 사용 (`O(N)`)
    

> 객체 정렬은 비교 비용이 비싸기 때문에  
> **안정성과 예측 가능한 성능**이 더 중요

---
3. `Collections.sort()`

`Collections.sort(List<T>)`

내부 구현

👉 **List를 배열로 변환 → TimSort 사용**

즉,

- `Arrays.sort(Object[])`와 동일한 알고리즘
    
- **항상 안정 정렬**
    

---
4. 왜 primitive와 object를 다르게 설계했을까?

핵심 이유

1. **primitive**
    
    - 비교 연산이 매우 빠름
        
    - 메모리 접근 패턴 중요
        
    - → 퀵소트가 유리
        
2. **object**
    
    - 비교 시 `compareTo()` / `Comparator` 호출
        
    - 상대적으로 비용 큼
        
    - → 안정성 + 최악 성능 보장이 중요
        

---
5. 한눈에 정리

|API|대상|알고리즘|안정성|
|---|---|---|---|
|`Arrays.sort(int[])`|primitive|Dual-Pivot QuickSort|❌|
|`Arrays.sort(Object[])`|object|TimSort|✅|
|`Collections.sort()`|List|TimSort|✅|

##### 정렬해야 하는 데이터는 50G 인데, 메모리가 4G라면, 어떤 방식으로 정렬을 진행할 수 있을까요?

이 상황은 전형적인 **외부 정렬(External Sorting)** 문제입니다.  
한 줄 요약하면 **“메모리에 올릴 수 있는 만큼씩 나눠 정렬 → 디스크에서 병합”** 입니다.

---
1. 왜 내부 정렬이 불가능한가

- 정렬 대상: **50GB**
    
- 사용 가능 메모리: **4GB**
    

👉 전체 데이터를 한 번에 메모리에 올릴 수 없음  
👉 `Arrays.sort`, 퀵소트, 머지소트 같은 **내부 정렬(in-memory sort)** 은 사용 불가

---

2. 해법: External Merge Sort (외부 머지소트)

전체 전략 (2 Phase)

1. **Run 생성 단계 (분할 + 내부 정렬)**
    
2. **K-way 병합 단계**
    

이 방식은 DB, Hadoop, Spark, Unix `sort` 명령까지  
**대용량 정렬의 표준 해법**입니다.

---
3. Phase 1: Run 생성 (Chunk Sort)

절차

1. 디스크에서 **메모리에 올라갈 수 있는 크기만큼** 읽기  
    → 예: 1GB 단위
    
2. 메모리 안에서 정렬  
    → 퀵소트 / TimSort
    
3. 정렬된 결과를 **임시 파일(run)** 로 디스크에 저장
    
4. 전체 데이터가 끝날 때까지 반복
    
예시

- 50GB ÷ 1GB = **50개의 정렬된 run 파일**
    

`run_01.dat run_02.dat ... run_50.dat`

👉 이 단계는 **순차 I/O 중심**이라 디스크 효율이 좋습니다.

---
4. Phase 2: K-way Merge (다중 병합)
아이디어

- 여러 개의 정렬된 run을 **동시에 조금씩 읽어** 병합
    
- **우선순위 큐(PriorityQueue)** 사용
    
병합 방식

1. 각 run에서 버퍼(예: 8MB)씩 메모리에 로드
    
2. 각 버퍼의 “현재 최소값”을 PQ에 넣음
    
3. 가장 작은 값 출력 → 해당 run에서 다음 값 로드
    
4. 버퍼 소진 시 디스크에서 다음 블록 읽기
    

---
5. 메모리 4GB에서의 병합 전략

메모리 사용 예:

- 입력 버퍼: run 당 8MB × 400개 ≈ 3.2GB
    
- 출력 버퍼: 100MB
    
- 여유 메모리 확보
    

👉 **한 번에 병합 가능한 run 수(K)** 는

> `K ≈ 메모리 / 버퍼크기`

만약 run이 너무 많다면?

👉 **Multi-pass merge**

- 50개 → 10개씩 병합 → 5개 → 최종 병합
    

---

